<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://klasmodin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://klasmodin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-31T16:37:37+00:00</updated><id>https://klasmodin.github.io/feed.xml</id><title type="html">blank</title><subtitle>Web-page for mathematician Klas Modin. </subtitle><entry><title type="html">What is the Matrix?</title><link href="https://klasmodin.github.io/blog/2025/what-is-the-matrix/" rel="alternate" type="text/html" title="What is the Matrix?"/><published>2025-03-17T07:00:00+00:00</published><updated>2025-03-17T07:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2025/what-is-the-matrix</id><content type="html" xml:base="https://klasmodin.github.io/blog/2025/what-is-the-matrix/"><![CDATA[ <div class="row justify-content-center"> <div class="col-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neo-sleeping-480.webp 480w,/assets/img/neo-sleeping-800.webp 800w,/assets/img/neo-sleeping-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/neo-sleeping.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="turbulence" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>“This movie will change my life,” I thought, and with goosebump on my arms I left the cinema. Deep in thoughts, I began walking home in the direction I believed was right. The city of Lund was new to me; I was a university freshman at the beginning of the first semester.</p> <p>I halted in front of the <a href="https://en.wikipedia.org/wiki/Lund_University_Main_Building#/media/File:Universitetsbyggnaden_080508.jpg">white university building</a>, which was accentuated by the twilight. The building excited and frightened me; it stood there as a token of knowledge, yet it was bewildering, as the night was. That evening I had watched <a href="https://en.wikipedia.org/wiki/The_Matrix">The Matrix</a>, and it did, in a way, change my life.</p> <p>When it premiered, The Matrix was celebrated for its special effects, which undeniably boosts it. But the special effects aren’t the reason for its later cult. No, the entrancement is in the theme, combining the ancient philosophical question, “Is the world different from what it seems?”, with the awe over internet in the era of 56k dial-up modems.</p> <p>The game-changing scene in The Matrix is when Neo, the computer hacker protagonist, first meets Trinity, at an underground nightclub. As the music turns edgy, the alluring storyline becomes irresistible when Trinity whispers to Neo:</p> <p>“I know why you’re here Neo. I know why you hardly sleep, why you live alone, and why night after night you sit at your computer. I know, because I was once looking for the same thing. […] It’s the question that drives us, Neo. You know the question, just as I did.”</p> <p>“What is the Matrix?”</p> <p>From Neo’s reply, I vaguely recalled the number tables called <em>matrices</em> we’d learned about in high-school mathematics, but I didn’t see a connection to the movie. Yet my curiosity was awoken.</p> <p style="text-align: center;">~</p> <p>In high-school, I had mixed feelings about mathematics. It was unsatisfactory to memorize algorithms and apply them to variations of poorly motivated equations. But <em>programming</em> was fun. I’d been hooked since my father had bought a <a href="https://en.wikipedia.org/wiki/Macintosh_Plus">Macintosh Plus</a> in the late ’80s and had taught me to write simple programs in <a href="https://en.wikipedia.org/wiki/ZBasic">ZBasic</a>.</p> <p>During high-school lessons we competed to write demo-programs on our <a href="https://en.wikipedia.org/wiki/TI-82">TI-82</a> calculators. The king of the hill was a classmate nicknamed “Hacker-Björn”. One morning he kept energetically tapping his calculator while refusing to reveal anything. As he finally and triumphantly announced his demo, we gathered over his calculator like a flock of hungry gulls. He unveiled a smooth animation resembling the twisting of DNA. It was beautiful, and the way he had programmed it was remarkable. The rest of us had relied on the TI-82’s lagging graphics engine, but Hacker-Björn used simple ASCII output to produce fast, animated graphics. I was so impressed and jealous. It was an extraordinary lesson of thinking outside the box.</p> <p>After high-school, I moved to the south of Sweden to study engineering at Lund University. During the first year, soon after I had watched The Matrix, I began a course in linear algebra. Our teacher was <a href="https://portal.research.lu.se/en/persons/magnus-fontes">Magnus Fontes</a> and he was brilliant. With wits, he filled our minds with vectors and linear transformations, represented by <em>matrices</em>.</p> <p>Once he gave a motivation lecture, where he illustrated how his own research (in <a href="https://en.wikipedia.org/wiki/Harmonic_analysis">harmonic analysis</a>) applies to the brewing of beer and to synthesizers in electronic music. Suddenly, in-between sentences, he said:</p> <p>“It’s a well-hidden secret among math professors that most problems can’t be solved.”</p> <p>These words grew in me over the years. They evolved into a mantra, applicable to mathematics, science, and life in general: “The more I learn, the more I realize I don’t know.” There’s beauty in this awareness; it’s a whisper of unlimited possibilities.</p> <p>The course in Linear Algebra changed my view of mathematics. From there on, it wasn’t about memorizing algorithms. Linear Algebra is a self-contained theory. It’s abstract, yet concrete because it’s geometric and therefore evokes mental pictures. Soon after the course ended, I decided to quit engineering for full-time studies in mathematics.</p> <p style="text-align: center;">~</p> <p>Twenty years later, <a href="https://www.sns.it/it/persona/milo-viviani">Milo Viviani</a> and I began to learn how geometry manifests itself in the equations of hydrodynamics. These equations were formulated by <a href="https://en.wikipedia.org/wiki/Leonhard_Euler">Leonhard Euler</a> in 1757 but are still far from understood. Somehow, the more mathematicians learn about them, the more peculiar their solutions appear to be. Yet, there are ways to approach them.</p> <p>In 1854, <a href="https://en.wikipedia.org/wiki/Bernhard_Riemann">Bernard Riemann</a> developed a framework for describing generalized, higher-dimensional surfaces. Today we call this framework <em>Riemannian geometry</em>, and it’s a cornerstone of modern mathematics. <a href="https://en.wikipedia.org/wiki/Vladimir_Arnold">Vladimir Arnold</a> realized in 1966 that Riemannian geometry also describes Euler’s equations of hydrodynamics. The discovery is important because it maps mental pictures of geometry to intuition about hydrodynamics. A surface has curvature. What’s the curvature of hydrodynamics? What does it mean that hydrodynamics has curvature?</p> <p>Consider a mountaineer walking along <em>geodesics</em>. That is, she walks along a path such that whenever she pauses to look back at a point she passed before, she couldn’t have made it from that point to where she is via a shorter path. Imagine now two such mountaineers walking next to each other along a mountain ridge shaped like a saddle. The ridge has negative curvature, which implies that the mountaineers eventually diverge from each other, ending up on different sides of the mountain. On the other hand, if they were walking in a crater, or near the top of a hill, where curvature is positive, they’d converge towards each other and eventually cross paths. Thus, geodesic motion is stable (i.e., converging) where curvature is positive and unstable (i.e., diverging) where it’s negative.</p> <p>Now take a leap of thought: just as the curvature of a two-dimensional surface signify stability, the curvature of hydrodynamics reveals stability of fluid motion. This line of reasoning exemplifies the power of mathematics.</p> <p>A mathematical equation is blind to its applications. It’s a blueprint of a perfect machine, potentially useful but with no predestined purpose.</p> <p>Arnold’s discovery showed that two previously distinct areas of mathematics were connected. (i) Euler derived the hydrodynamic equations, (ii) Riemann generalized geometry, (iii) Arnold saw the former in the latter. And these events happened one century apart. Fascinating!</p> <p>We cannot write down a general formula that solves Euler’s equations, so we need other ways to latch on to their solutions. One possibility is to use “approximate” equations which we <em>can</em> solve via computer algorithms. But Euler’s equations are evasive, as though designed to keep their solutions secret. There are computer algorithms that approximate solutions on short time intervals, but they fail on longer time intervals. And “longer” is typically still quite short. A different strategy is needed.</p> <p>In 1890, in his work on the stability of the solar system, <a href="https://www.mittag-leffler.se/about-us/history/prize-competition/">Henri Poincaré</a> laid down a new strategy to understand dynamical systems: stop focusing on individual solutions and instead study the qualitative behavior of generic solutions. And here Arnold’s geometry enters, because it unravels qualitative features and thereby enables Poincaré’s approach for the study of Euler’s equations. But for computer generated approximations there is still a problem: standard computer algorithms fail to preserve the geometry, so on long time intervals the qualitative properties disintegrate. Viviani and I asked if there are computer algorithms that approximate solutions of Euler’s equations in such a way that Arnold’s geometric description remains intact. I eventually remembered from my post-doc time in New Zealand that <a href="https://www.massey.ac.nz/massey/expertise/profile.cfm?stref=677230">Robert McLachlan</a> had showed me a curious way to approximate the 2-D Euler equations via something called the <em>sine-bracket</em>. I returned to it and learned that it’s a method developed by Vladimir Zeitlin in 1991 which indeed preserves the geometric structure, and which at heart uses quantization theory to replace the continuous vector field in Euler’s equations with a <em>matrix</em>. My excitement went off the charts.</p> <p>We plowed the literature for follow-up studies on Zeitlin’s model, and we found – not so much. While the method preserves Arnold’s geometry, it approximates individual solutions worse than standard algorithms and therefore didn’t catch on in the computational mathematics community. But we weren’t bothered, since individual solutions can’t be approximated for long times anyway, regardless of the choice of algorithm. Instead, we set out to further study Zeitlin’s model, which clearly was underexplored (and still is).</p> <p>The beauty of Zeitlin’s model, or <em>matrix hydrodynamics</em>, is that it gives a mathematical dictionary between matrix theory and hydrodynamics, which in turn enables new theory about matrices for addressing old questions about hydrodynamics. Furthermore, computers are superb at matrix calculations, so we can explore theoretical ideas via computer experiments. At the same time, the map from a fluid velocity field to a matrix is hard to grasp. What do the matrix elements really mean? Indeed, these days I often find myself late at night in front of the computer, half asleep, while thinking: “What is the matrix?”</p> ]]></content><author><name>Klas Modin</name></author><category term="undergraduate"/><category term="hydrodynamics"/><category term="popular"/><summary type="html"><![CDATA[a movie and hydrodynamics]]></summary></entry><entry><title type="html">Simulation of Kähler potentials</title><link href="https://klasmodin.github.io/blog/2025/kahler-potentials/" rel="alternate" type="text/html" title="Simulation of Kähler potentials"/><published>2025-02-22T12:00:00+00:00</published><updated>2025-02-22T12:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2025/kahler-potentials</id><content type="html" xml:base="https://klasmodin.github.io/blog/2025/kahler-potentials/"><![CDATA[<p>For a given probability density \(\sigma\), consider the vorticity-type equations</p> \[\dot\rho + \{\psi,\rho\}=0, \qquad \Delta \psi = \rho -\sigma\] <p>where $\rho$ is a probability density and $\psi$ is the stream function. We can discretize these equations via matrix hydrodynamics, which yields the isospectral matrix flow</p> \[\dot R = \frac{1}{\hbar}[P,R],\qquad \Delta_N P = R - F .\] <p>It is interesting to compare long-time simulations of such systems with the predictions from statistical mechanics (corresponding to optimal potentials in Kähler geometry).</p> <p>We randomly generate three background measures \(R\) (via a spherical harmonics expansion with maximum wave-number \(\ell=3\)). Then we use random initial conditions (with maximum wave-number \(\ell=10\)) and run the simulations. The results are given below (to the left is the background measure \(R\)).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/berman_shift_F_seed_1_N_512-480.webp 480w,/assets/img/berman_shift_F_seed_1_N_512-800.webp 800w,/assets/img/berman_shift_F_seed_1_N_512-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/berman_shift_F_seed_1_N_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/berman_shift_seed_1_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/berman_shift_F_seed_2_N_512-480.webp 480w,/assets/img/berman_shift_F_seed_2_N_512-800.webp 800w,/assets/img/berman_shift_F_seed_2_N_512-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/berman_shift_F_seed_2_N_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/berman_shift_seed_2_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/berman_shift_F_seed_3_N_512-480.webp 480w,/assets/img/berman_shift_F_seed_3_N_512-800.webp 800w,/assets/img/berman_shift_F_seed_3_N_512-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/berman_shift_F_seed_3_N_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/berman_shift_seed_3_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/berman_shift_F_seed_4_N_512-480.webp 480w,/assets/img/berman_shift_F_seed_4_N_512-800.webp 800w,/assets/img/berman_shift_F_seed_4_N_512-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/berman_shift_F_seed_4_N_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/berman_shift_seed_4_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <p>Observations:</p> <ul> <li> <p>In most simulations, the mass gets concentrated around the strongest stationary blobs.</p> </li> <li> <p>In one simulation (a single blob found an “integrable-like” motion)</p> </li> </ul>]]></content><author><name></name></author><category term="internal"/><summary type="html"><![CDATA[Preliminary results on spherical simulations corresponding to statistical mechanics for Kähler potentials]]></summary></entry><entry><title type="html">Magnetohydrodynamics with viscosity</title><link href="https://klasmodin.github.io/blog/2024/viscous-mhd/" rel="alternate" type="text/html" title="Magnetohydrodynamics with viscosity"/><published>2024-09-21T12:00:00+00:00</published><updated>2024-09-21T12:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2024/viscous-mhd</id><content type="html" xml:base="https://klasmodin.github.io/blog/2024/viscous-mhd/"><![CDATA[<p>The 2-D magnetohydrodynamic (MHD) equations can be formulated in terms of a fluid vorticity field \(\omega\) and a magnetic vorticity field \(\theta\)</p> \[\dot\omega = \{-\psi, \omega\} + \{-\beta, \theta\} +\nu \Delta\omega\] \[\dot\theta = \{-\psi, \theta\} \phantom{+ \{-\beta, \theta\} +\nu \Delta\omega}\] <p>where \(\nu \geq 0\) is the viscosity (acting as a dissipative force on the fluid vorticity only), \(\psi = \Delta^{-1}\omega\) is the stream function, and \(\beta = \Delta\theta\) is the magnetic stream function. Geometrically, this is a Lie-Poisson system on a <em>magnetic extension</em> semi-direct product Lie algebra \(\mathfrak{g}\ltimes\mathfrak{g}^*\). The Hamiltonian for the system is given by</p> \[H(\omega,\theta) = \frac{1}{2}\int_{S^2} (\omega\Delta^{-1}\omega + \theta\Delta\theta)\mu .\] <p>where \(\mu\) is the symplectic form (likewise the area-element) of the sphere \(S^2\). We can split the energy into two components, corresponding to the variables \(\omega\) and \(\theta\):</p> \[E_1(\omega) = \frac{1}{2}\int_{S^2} \omega\Delta^{-1}\omega\; \mu .\] <p>and</p> \[E_2(\theta) = \frac{1}{2}\int_{S^2} \theta\Delta\theta\;\mu .\] <p>The corresponding Zeitlin-MHD model is then given by</p> \[\dot W = \frac{1}{\hbar}[-P, W] {+\frac{1}{\hbar}[-B, \Theta]} {+\nu \Delta_N W}\] \[\dot \Theta = \frac{1}{\hbar}[-P, \Theta] \phantom{+ \frac{1}{\hbar}[M,\Theta] + \nu \Delta_N W}\] <p>where \(\Delta_N P = W\) and \(B = \Delta_N \Theta\), for the Hoppe-Yau Laplacian \(\Delta_N\).</p> <p>To run the simulation, let’s use random, smooth initial \(W_0\) and \(\Theta_0\) distributed the same, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/viscous-mhd-W0-480.webp 480w,/assets/img/viscous-mhd-W0-800.webp 800w,/assets/img/viscous-mhd-W0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/viscous-mhd-W0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/viscous-mhd-Theta0-480.webp 480w,/assets/img/viscous-mhd-Theta0-800.webp 800w,/assets/img/viscous-mhd-Theta0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/viscous-mhd-Theta0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I run the simulation for long time with \(N=128\) for the viscosities \(\nu=1\), \(\nu = 0.1\), and \(\nu=0.01\). Whereas \(W\) converges to zero, the results indicate that the generic behavior for \(\Theta\) is to settle at two blobs, and that this point is reached faster for the smaller viscosity:</p> <div class="row mt-2"> <div class="col-sm mt-2"> <figure> <video src="/assets/video/mhd_nu_1_0_Theta.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-2"> <figure> <video src="/assets/video/mhd_nu_0_1_Theta.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-2"> <figure> <video src="/assets/video/mhd_nu_0_01_Theta.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Animations of theta: left: nu=1.0, middle: nu=0.1, right: nu=0.01 </div> <p>The total energy decreases in time, which follows from the calculation</p> \[\frac{d}{dt} H = -\mathrm{Tr}(\dot W P) - \mathrm{Tr}(\dot\Theta B) = \frac{1}{\hbar}\mathrm{Tr}(P([P, W] +[B, \Theta] -\nu \Delta_N W)) + \frac{1}{\hbar}\mathrm{Tr}(B[P,\Theta])\] \[= -\frac{\nu}{\hbar}\mathrm{Tr}(\Delta_N^{-1}W \Delta_N W) = - \frac{\nu}{\hbar} \lVert W \rVert_F^2 .\] <p>But the partial energies \(E_1\) and \(E_2\) are not necessarily monotone. For the \(\nu=0.01\) simulation their evolution looks as follows:</p> <div class="row justify-content-center"> <div class="col-12 col-sm-10"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/viscous-mhd-partial-energies-480.webp 480w,/assets/img/viscous-mhd-partial-energies-800.webp 800w,/assets/img/viscous-mhd-partial-energies-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/viscous-mhd-partial-energies.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="internal"/><summary type="html"><![CDATA[Numerical results for the Zeitlin model applied to spherical MHD with viscosity]]></summary></entry><entry><title type="html">Zeitlin model for beta-SQG</title><link href="https://klasmodin.github.io/blog/2023/sqg-zeitlin/" rel="alternate" type="text/html" title="Zeitlin model for beta-SQG"/><published>2023-12-05T12:00:00+00:00</published><updated>2023-12-05T12:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2023/sqg-zeitlin</id><content type="html" xml:base="https://klasmodin.github.io/blog/2023/sqg-zeitlin/"><![CDATA[<p>For \(\beta\in [0,1]\), an interpolation between 2-D Euler and SQG is given by</p> \[\dot\omega = \{\psi,\omega\}, \quad \Delta^{1-\beta/2}\psi = \omega \; .\] <p>The corresponding Zeitlin model on \(\mathfrak{su}(N)\) is given by</p> \[\dot W = \frac{1}{\hbar} [P,W], \quad \Delta_N^{1-\beta/2} P = W \; .\] <p>Here, we compute the inverse of the rational Laplacian via the spherical harmonic basis: if \(W = \sum_{\ell=0}^{N-1}\sum_{m=-\ell}^\ell\omega_{\ell m} T^N_{\ell m}\) then</p> \[P = \sum_{\ell=0}^{N-1}\sum_{m=-\ell}^\ell \frac{\omega_{\ell m}}{((\ell+1)\ell)^{1-\beta/2}} T^N_{\ell m} \; .\] <p>I specify a random, but smooth initial \(W_0\) with \(\omega_{1 m} = 0\) (vanishing angular momentum). It looks as follows:</p> <div class="row justify-content-center"> <div class="col-10 col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zeitlin-sqg-W0-480.webp 480w,/assets/img/zeitlin-sqg-W0-800.webp 800w,/assets/img/zeitlin-sqg-W0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zeitlin-sqg-W0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Physically, it’s kind of awkward to select the same initial vorticity for different choices of \(\beta\), as it implies that the initial velocities (or, equivalently, stream functions) are different. However, geometrically it makes sense, since all the simulations then evolve on the same co-adjoint orbit.</p> <p>I run the simulation for long time with \(N=512\) for \(\beta = 0, 0.25, 0.5, 1.0\). The Euler case (\(\beta=0\)) yields the expected results: the formation of 4 interacting blobs. The other simulations show a similar asymptotic behavior: a final state of 4 interacting blobs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/sqg2_beta_0_25_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/sqg2_beta_0_5_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/sqg2_beta_1_0_N_512.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Left: beta=0.25, Middle: beta=0.5, Right: beta=1.0 </div> <p>Notice that the mixing phase is a little slower for larger \(\beta\), probably because the blobs are “sharper”. Theo Drivas hinted to me that this would be the case. I don’t have a good intuition for this mechanism yet, other than that we expect less regularity. Actually, the lower regularity is well reflected in the spectral energy plot:</p> <div class="row justify-content-center"> <div class="col-12 col-sm-10"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sqg-energy-spectrum.svg" sizes="95vw"/> <img src="/assets/img/sqg-energy-spectrum.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Observations:</p> <ul> <li> <p>For all \(\beta\) we get a broken line corresponding to an inverse energy cascade.</p> </li> <li> <p>The \(\beta=1\) is the limiting case when the slope is flat.</p> </li> <li> <p>All slopes intersect in approximately the same point. This must correspond to the simulations evolving on the same co-adjoint orbit.</p> </li> </ul>]]></content><author><name></name></author><category term="internal"/><summary type="html"><![CDATA[Preliminary results on applying Zeitlin discretization to beta-SQG]]></summary></entry><entry><title type="html">The reversibility paradox in Zeitlin’s model</title><link href="https://klasmodin.github.io/blog/2023/zeitlin-reversibility/" rel="alternate" type="text/html" title="The reversibility paradox in Zeitlin’s model"/><published>2023-08-24T07:00:00+00:00</published><updated>2023-08-24T07:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2023/zeitlin-reversibility</id><content type="html" xml:base="https://klasmodin.github.io/blog/2023/zeitlin-reversibility/"><![CDATA[<p>During the pandemic, I gave an online seminar where I unveiled some numerical simulations of incompressible 2-D hydrodynamics on the sphere, which indicated a connection between the long-time behavior of 2-D Euler equations and integrability conditions for point-vortex dynamics (as I shall explain below). After the seminar, Anton Izosimov and Boris Khesin asked me an insightful question:</p> <p>The Lie group \(\mathrm{SU}(N)\), underlying the model for the simulations, is compact. Because the dynamics in the model is also Hamiltonian, we have Poincaré recurrence. But the dynamics in the simulations, leading to blob formations, seem contractive. Isn’t the mechanism for blob formations instead induced by fictitious dissipation, introduced via the numerical time-discretization?</p> <p>I didn’t have a good answer at the time, but the question stayed with me. Today I have an answer – the subject of the post. Before I reveal it, however, we shall need some background.</p> <h2 id="boltzmann-versus-poincaré">Boltzmann versus Poincaré</h2> <p>In 1872 Ludwig Boltzmann published his <a href="https://en.wikipedia.org/wiki/H-theorem">H-theorem</a>. It states that, in a closed system of interacting, idealized gas particles, <strong>entropy increases with time</strong>. Some time later, in 1888, Henri Poincaré formulated and proved the <a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_recurrence_theorem">recurrence theorem</a> as part of his quest to secure the <a href="https://www.mittag-leffler.se/about-us/history/prize-competition/">prize competition</a> organized by Gösta Mittag-Leffler in honor of King Oscar II of Sweden (a remarkable episode in the history of science, but one I shall not elaborate on here). Since then, the two results had widespread influences on subsequent developments in mathematics and physics. The former is the hallmark of <a href="https://en.wikipedia.org/wiki/Thermodynamics">thermodynamics</a> and <a href="https://en.wikipedia.org/wiki/Statistical_mechanics">statistical mechanics</a>, and is considered by some scientists the <a href="https://www.quantamagazine.org/physicists-trace-the-rise-in-entropy-to-quantum-information-20220526/">most fundamental principle of modern physics</a> (although still far from well understood). The latter led to the modern mathematical theory of dynamical systems, to ergodic theory, and to the concept of homoclinic orbits and chaos. At the same time, the juxtaposition of the two results brought about one of the most intriguing debates in modern science: <strong>the reversibility paradox</strong>. How can a system governed by reversible laws of motion give rise to seemingly irreversible behavior?</p> <div class="row justify-content-center"> <div class="col-8 col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gas-particles-in-box.svg" sizes="95vw"/> <img src="/assets/img/gas-particles-in-box.svg" class="img-fluid" width="100%" height="auto" title="shapes" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A closed box with gas particles initially in the first chamber </div> <p>Here is the classical illustration of the paradox, pointed out in 1876 by <a href="https://en.wikipedia.org/wiki/Johann_Josef_Loschmidt">Johann Josef Loschmidt</a>, in relation to time-reversibility, and in 1896 by <a href="https://en.wikipedia.org/wiki/Ernst_Zermelo">Ernst Zermelo</a>, in relation to recurrence. Consider a closed box of finite volume, containing two chambers with an opening between them. Now populate the first chamber with a large but finite number of gas particles that interact in a pair-wise, time-reversible fashion according to Newtonian mechanics. As the system is released, the gas particles begin to diffuse into the second chamber. According to Boltzmann’s H-theorem, they continue doing so until a macroscopic equilibrium is reached, in which the particles on average are distributed uniformly across the two chambers. On the other hand, according to Poincaré’s recurrence theorem, for each choice of \(\delta&gt;0\), there is a finite time \(t_\delta\) at which each particle has returned to within a distance \(\delta\) of its initial position and velocity. In particular, if one waits long enough, the particles should eventually come to a configuration where they all reside in the first chamber. But, according to the H-theorem, that cannot happen, since such a macroscopic state has lower entropy than a mixed chamber state. Furthermore, no-one has ever witnessed anything like that in experiments. And so the paradox arises.<d-footnote>An entertaining, popular account of the entropy law and the controversies it brought can be found in the book <i>Mysteries of Modern Physics: Time</i> by Sean Carrol.</d-footnote></p> <p>Of course, it is not truly a “paradox”. Poincaré’s recurrence theorem is undoubtedly valid under its assumptions (volume preservation of the phase flow and boundedness of the phase orbits), whereas the H-theorem is a probabilistic statement that requires undesirable assumptions to be proved rigorously. In this sense, Poincaré’s result has the upper hand. But, as mentioned, it is Boltzmann’s entropy law that physicists over and over again confirm in experiments, that we observe in our everyday life (you never see an egg “un-crack”), and that successfully explains the <a href="https://en.wikipedia.org/wiki/Arrow_of_time">arrow of time</a> and the <a href="https://en.wikipedia.org/wiki/Past_hypothesis">past hypothesis</a> in cosmology. In short, despite the rigorousness of the recurrence theorem, the behavior predicted by the entropy law is more ubiquitous. How, then, can we explain that we never see the gas particles returning to the first chamber?</p> <p>An obvious thesis is that the governing laws of motion for real gas particles are more complicated than the Newtonian particles in an idealized gas. This argument is correct, but it does not get to the heart of the matter. Indeed, in computer simulations it is possible to <em>exactly</em> model the motion of a <a href="https://en.wikipedia.org/wiki/Hard_spheres">hard sphere gas</a>. In such simulations, one <em>never</em> observes a return of the particles to the first chamber. Yet, the conditions of the recurrence theorem are fulfilled, so there <em>must</em> be a time \(T\) at which the particles return. Herein lies the crux of the issue.</p> <p>The resolution is that \(T\) is <em>enormously large</em> – much larger than the age of the universe. Let us estimate it.</p> <p>Assume that the closed box is \(10\mathrm{cm}\times 10\mathrm{cm}\times 10\mathrm{cm}\), with two chambers of equal volume. The average kinetic energy of a monatomic gas particle is given by \(3/2 k_B T\), where \(k_B\) is the Boltzmann constant and \(T\) is the absolute temperature. For helium at <a href="https://en.wikipedia.org/wiki/Standard_conditions_for_temperature_and_pressure">standard temperature</a>, this gives an average particle speed of \(\bar v \approx 1300~\mathrm{m/s}\). The total number of particles at standard temperature and pressure is about \(2\cdot 10^{22}\), but we would like to work with a very low density of particles, so let us take \(N=10^{20}\). The length of each chamber is \(5\mathrm{cm}\), so, depending on how the barrier between the chambers is set up, the average “escape time” for a particle is at least \(0.05/1300 \approx 4\cdot 10^{-5}\,\mathrm{s}\). We therefore sample the state of all the particles every \(10^{-5}\,\mathrm{s}\). We further assume that the location of each particle is uncorrelated to its position at the previous sample (this is likely false, but anyway makes the recurrence time shorter and is therefore safe). Finally, we assume that the particle density is so low that the particles are uncorrelated. Since the chambers have equal volume, the probability of finding particle \(i\) in the first chamber at any sample is thereby 1/2. Consequently, the expected number of samples until all particles are simultaneously in the first chamber is \(2^{N}\), which corresponds to about \(10^{(\log_{10} 2)N-11} &gt; 10^{10^{19}}\) years. That is a long time to wait. The age of the universe is about \(10^{10}\) years. Even in a modest computer simulation, for a very small box corresponding to \(N=10^5\) particles, we would have to simulate for \(&gt; 10^{10^4}\) years (assuming the simulation can run in real-time). To summarize, we see that the Poincaré recurrence theorem does not yield practically applicable predictions, unless the phase space dimension is tiny. It nevertheless provides many useful theoretical insights, for example the impossibility of a continuous, strictly definite <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov function</a>.</p> <p>All this is fine, and known since long, but how is it related to Zeitlin’s model of hydrodynamics? What <em>is</em> Zeitlin’s model anyway?</p> <h2 id="zeitlins-model-in-a-nutshell">Zeitlin’s model in a nutshell</h2> <p>The Euler equations on a 2-dimensional Riemannian manifold \(M\) are given by</p> <p>\begin{equation}\label{eq:euler2d} \dot\omega = \{\psi, \omega \},\qquad \Delta\psi = \omega, \end{equation}</p> <p>where \(\omega\) is the <em>vorticity</em> and \(\psi\) is the <em>stream function</em>, related to the fluid velocity \(v\) via the skew-gradient \(v = \nabla^{\bot}\psi\).</p> <p>The <em>Poisson bracket</em>, given by \(\{f,g \} = \nabla^\bot f \cdot \nabla g\), makes \(C^\infty(M)\) into an infinite-dimensional Lie algebra. Thus, the geometric structures needed to define the 2-D Euler equations are</p> <ol> <li>a Lie algebra \(\mathfrak g = (C^\infty(M),\{\cdot,\cdot\})\) (the pre-dual phase space), and</li> <li>a Laplacian operator \(\Delta: C^\infty(M)\to C^\infty(M)\).</li> </ol> <p>This rich geometric structure gives rise to many conservation laws. In particular,</p> <ul> <li>the phase flow preserves a <em>Lie-Poisson structure</em> <d-cite key="Ar1989"></d-cite>;</li> <li>the <em>Hamiltonian function</em> \(H(\omega) = \int_M \omega\psi\) is conserved;</li> <li>there are infinitely many additional conserved quantities, called <em>Casimir functions</em>, given by \(C_k(\omega) = \int_M \omega^k\);</li> <li>the phase flow is the reduced version of a geodesic flow on the infinite-dimensional group of area-preserving diffeomorphisms of \(M\) <d-cite key="Ar1966"></d-cite>.</li> </ul> <p>In 1991, Vladimir Zeitlin <d-cite key="Ze1991"></d-cite> had the brilliant idea to construct an approximation to the 2-D Euler equations \eqref{eq:euler2d} by specifying a sequence of Lie algebras \(\mathfrak g_N\) so that (in some sense) \(\mathfrak g_N \to \mathfrak g\) as \(N\to\infty\). He had, to his help, results from quantization theory, in particular those derived by Jens Hoppe <d-cite key="Ho1989"></d-cite> a few years earlier, which exactly gave such an approximation (but without reference to the Euler equations). For the flat 2-torus \(M=\mathbb T^2\), Zeitlin thereby constructed a finite-dimensional model of the 2-D Euler equations evolving on \(\mathfrak g_N = \mathfrak{su}(N)\). This gives rise to a matrix flow analogous to the 2-D Euler equations \eqref{eq:euler2d}</p> <p>\begin{equation}\label{eq:zeitlin} \dot W = \frac{1}{\hbar}[P, W ],\qquad \Delta_N P = W, \end{equation}</p> <p>where \(\hbar = 1/N\) is a normalizing factor, \(W\in \mathfrak{su}(N)\) is the <em>vorticity matrix</em>, and \(P \in \mathfrak{su}(N)\) is the <em>stream matrix</em>. The operator \(\Delta_N:\mathfrak{su}(N)\to \mathfrak{su}(N)\) is the <em>quantized Laplacian</em>. As mentioned, Zeitlin originally considered a version of \(\Delta_N\) that corresponds to \(M=\mathbb{T}^2\). However, it turns out that the approximation works even better for the sphere \(M=\mathbb{S}^2\), due to the special structure of \(\Delta_N\) in this case (corresponding to the symmetric space and Kähler structure of the sphere, <em>cf.</em> Hoppe and Yau <d-cite key="HoYa1998"></d-cite>). The Zeitlin flow \eqref{eq:zeitlin} is <em>isospectral</em>, so the spectrum of \(W\) is conserved. This corresponds to the conservation of Casimirs. Indeed, the quantized Casimirs are given by \(C_k(W) = \mathrm{tr}(W^k)\).</p> <p>At about 2016, in my work with Milo Viviani, we began a special study of Zeitlin’s model. We quickly realized it is under-explored, probably due to the difficulty to construct a numerically feasible algorithm for the projection \(C^\infty(\mathbb{S}^2)\to \mathfrak{su}(N)\) and the corresponding inclusion \(\mathfrak{su}(N) \to C^\infty(\mathbb{S}^2)\). Overcoming this difficulty, we found all sorts of interesting phenomena (see <d-cite key="MoVi2020a"></d-cite><d-cite key="MoVi2020"></d-cite><d-cite key="MoVi2021"></d-cite><d-cite key="MoVi2022"></d-cite><d-cite key="CiViLuMoGe2022"></d-cite><d-cite key="CiViMo2022"></d-cite>). Perhaps most strikingly, there seems to be a connection between low-dimensional Hamiltonian “blob-dynamics” and the long-term mixing states. Indeed, our long-term simulations strongly suggest the following mechanisms for mixing:</p> <ol> <li>Smaller vortex formations of the same sign merge to larger formations when their trajectories come close enough (the inverse energy cascade).</li> <li>The motion of \(n\) vortex blobs is accurately described by \(n\) “fixed blob-shape” vortices as long as the blobs are well separated (so that no merging occurs).</li> <li>If the motion of \(n\) vortex blobs is not integrable, then, sooner or later, two vortex blobs of equal sign will reach a point in phase space where they are close enough to merge.</li> <li>If, however, the motion of the \(n\) vortex blobs is integrable (or at least close enough to integrable in the Kolmogorov–Arnold–Moser (KAM) sense) then the large-scale motion remains quasi-periodic with well-separated trajectories and no further merging occurs (integrability acts as a barrier in phase space, preventing further merging of blobs).</li> </ol> <p>This mechanism gives explicit predictions on how many blobs we expect to see: the largest \(n\) for which the dynamics is integrable. In the case \(M=\mathbb{S}^2\), we have \(n=3\) for generic angular momentum and \(n=4\) for vanishing angular momentum (see <d-cite key="MoVi2021"></d-cite>). Milo and I ran many simulations. All of them conform to this rule: whenever the angular momentum is (close to) zero we get 4 blobs interacting quasi-periodically, otherwise we get 3 blobs <d-cite key="MoVi2020"></d-cite> also interacting quasi-periodically. The videos below illustrate our conjecture (the sphere is visualized using the <a href="https://en.wikipedia.org/wiki/Hammer_projection">Hammer projection</a>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/nonvanishing2_forward_N_512_time_150_isofix.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/vanishing_forward_N_512_time_150_isofix.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Left: random initial conditions. The mixing settles at 3 blobs.<br/> Right: random initial conditions with vanishing angular momentum. The mixing settles at 4 blobs. </div> <h2 id="recurrence-in-zeitlins-model">Recurrence in Zeitlin’s model</h2> <p>Here is the precise statement of the recurrence theorem (continuous version):</p> <div class="fake-img l-body"> <p> Consider a dynamical system on a volume manifold \((M,\mu)\) with flow \(\varphi_t:M\to M\). If the flow is volume preserving, i.e., \(\varphi_t^*\mu = \mu\), and has only bounded orbits, then, for each open set, any orbit that intersects this open set intersects it infinitely often. </p> </div> <p>Let me now explain how the recurrent paradox comes up in Zeitlin’s model. The Euler–Zeitlin equations \eqref{eq:zeitlin} constitute, as mentioned, a Lie–Poisson system. As such, it evolves on the <em>co-adjoint orbit</em> defined by the initial conditions:</p> \[\mathrm{Orb}(W_0) = \{F^\dagger W_0 F \mid \varphi \in F\in \mathrm{SU}(N) \}.\] <p>Since \(\mathrm{SU}(N)\) is a compact Lie group, it follows that \(\overline{\mathrm{Orb}(W_0)}\) is compact for any \(W_0\in\mathfrak{su}(N)\). Furthermore, \(\mathrm{Orb}(W_0)\) is a <em>symplectic leaf</em> for the Poisson structure, which means it carries a natural symplectic structure preserved by the flow and compatible with the manifold structure of \(\mathrm{SU}(N)\). In turn, this symplectic structure induces a phase volume form on \(\mathrm{Orb}(W_0)\) also preserved by the flow. Thus, the phase flow on \(\mathrm{Orb}(W_0)\) is volume preserving and the orbits are bounded (since \(\overline{\mathrm{Orb}(W_0)}\) is compact), so the recurrence theorem is applicable.</p> <p>We conclude that, if we wait long enough, there should eventually appear a phase state visually indistinguishable from the initial state. In our simulations of the Zeitlin system, we <strong>never</strong> saw this phenomenon: we always end up with a mixed state of 3 or 4 interacting blobs, and no further large-scale mixing after that.</p> <p>But wait! Which dynamics do we actually see in our simulations? Any numerical time-stepping method implemented in the computer introduces numerical errors. Could the apparent convergence to blobs be a numerical artifact not present in the exact Euler–Zeitlin solutions? In other words, perhaps the convergence to blobs a consequence of the numerical time-discretization errors.</p> <h2 id="numerical-errors-in-a-nutshell">Numerical errors in a nutshell</h2> <p>Our simulations are based on a time-discretization given in a paper by Viviani and myself <d-cite key="MoVi2020a"></d-cite>. For the actual computer implementation, the numerical errors come in three flavors.</p> <p><strong>Local truncation errors</strong> <br/> At the first level, think of the time-discretization, for a time-step length \(h&gt;0\), as the iteration of a map \(\tilde\Phi_h:\mathbb{C}^{N\times N} \to \mathbb{C}^{N\times N}\) which is an approximation to the exact flow map \(\Phi_h\colon \mathfrak{su}(N)\to\mathfrak{su}(N)\). Contrary to the exact flow, the approximation \(\tilde\Phi_h\) is <em>not</em> (in general) a one-parameter flow map, i.e., \(\tilde\Phi_h\circ\tilde\Phi_h \neq \tilde\Phi_{2h}\). Now, each such sub-step \(W_{k+1} \mapsto \tilde\Phi_h(W_k) \equiv W_{k+1}\) gives rise to the local truncation error \(W_{k+1}-\Phi_h(W_k)\), where \(\Phi_h\) is the exact flow.</p> <p><strong>Root-finding error</strong> <br/> The map \(\tilde\Phi_h:W_k \mapsto W_{k+1}\) is typically not defined explicitly. In the case of our method, it is defined by the equations</p> \[W_k = (I- \frac{h}{2}P_{k+1/2})W_{k+1/2}(I + \frac{h}{2} P_{k+1/2}), \quad \Delta_N P_{k+1/2} = W_{k+1/2}\] \[W_{k+1} = (I + \frac{h}{2}P_{k+1/2})W_{k+1/2}(I - \frac{h}{2} P_{k+1/2}).\] <p>It is a system of non-linear equations for the unknowns \(W_{k+1/2}\) and \(W_{k+1}\) (only \(W_{k+1}\) is used in the next step). Such a system of equations is called a <em>numerical scheme</em>. To solve the equations on the computer, we use an iterative root-finding algorithm, for example fixed-point iterations or Newton’s method. Such an algorithm requires a stopping tolerance to decide if the iterate is close enough to the limiting element. Thus, we introduce another error whose size depends on the chosen tolerance.</p> <p><strong>Round-off errors</strong><br/> So far, we dealt with the numerical discretization as if it takes values in \(\mathbb{R}\) (or rather \(\mathbb{C}\)), which, of course, is false. Indeed, the computer can only represent a subset of \(\mathbb{R}\) given by the <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">floating-point numbers</a>. Since this subset is finite, and therefore not closed under arithmetic operations, any computer implementation gives rise to <a href="https://en.wikipedia.org/wiki/Round-off_error">round-off errors</a>. Typically, double precision floating-point arithmetics is used, in which round-off errors are small: for numbers of order 1, round-off errors in the arithmetic operations are of the order \(10^{-16}\). This is roughly the ratio between the hair’s breadth (\(\approx 15~\mu m\)) and the distance to the sun (\(\approx 1~\mathrm{au}\)).</p> <p>Numerical analysts, nowadays, are predominantly concerned with local truncation errors, since these typically dominate the total error. So, when you read in <a href="https://www.springer.com/journal/211">Numerische Mathematik</a> about the latest convergence result for some method, it really means convergence under the assumption of no root finding or round-off errors. There are, however, situations where those errors do play a role; long-time simulations of Zeitlin’s model is an example, as we shall see. But before I get to that issue, let me first show you an interesting numerical experiment.</p> <h2 id="first-experiment-times-arrow">First experiment: time’s arrow</h2> <p>Like every Hamiltonian system, the Euler–Zeitlin equations \eqref{eq:zeitlin} are <em>time-reversible</em> with respect to changing the sign of the Hamiltonian \(H(W)\). In other words, if \(\Phi_t\) is the flow of the system with Hamiltonian \(H\), and \(\Psi_t\) is the flow of the system with Hamiltonian \(-H\), then \(\Phi_{t} = \Psi_t^{-1}\).</p> <p>In the simulations above, however, it certainly looks like the dynamics is irreversible: we start from some randomly chosen initial \(W_0\) and from that we observe “convergence” to a blob configuration, possibly associated with an attractor. But, due to the Hamiltonian nature of the equations, there cannot exist such an attractor in the exact Zeitlin equations. Instead, if an attractor exists, it should come from <a href="https://en.wikipedia.org/wiki/Numerical_diffusion">numerical dissipation</a> introduced via the numerical errors.</p> <p>Let us now carry out the following experiment: for the vanishing momentum initial condition \(W_0\) as above, run one simulation for the Hamiltonian \(H\) (just as the second simulation above) and another one for \(-H\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/vanishing_forward_N_512_time_150_isofix.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/vanishing_reverse_N_512_time_150_isofix.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Left: forward simulation with random initial conditions.<br/> Right: the same initial conditions, but with time-reversed dynamics \(H\leftrightarrow -H \). </div> <p>Both simulations “convergence” to a 4 blob configuration. This behavior appears to be inconsistent with the time-reversible nature of the exact Euler–Zeitlin solutions: such a solution would start from a blob configuration at \(t \ll 0\), then unwrap itself to the smooth, non-blob configuration \(W_0\) at \(t=0\), only to again form a blob configuration at \(t \gg 0\). Consequently, numerical dissipation causes the blob formations, it seems.</p> <p>But, not too fast now, please keep reading.</p> <h2 id="reversible-and-symplectic-schemes">Reversible and symplectic schemes</h2> <p>In most situations, local truncation is the error source responsible for numerical dissipation. If we take, for example, the simplest Hamiltonian system, the harmonic oscillator, and discretize it by the <a href="https://en.wikipedia.org/wiki/Backward_Euler_method">backward Euler method</a>, we obtain, from the local truncation error, a numerical dissipation that can be re-interpreted as physical damping proportional to the stepsize \(h\). In contrast, however, any numerical dissipation in our method above, for the Euler–Zeitlin equations, cannot originate from local truncation errors. Let me explain why.</p> <p>If you study the scheme above, defining \(W_{k+1} = \tilde\Phi_h(W_k)\), notice that the equations are symmetric under the substitution \(P_{k+1/2}\leftrightarrow -P_{k+1/2}\). That is, if \(\tilde\Psi_h\) is the map \(W_k \mapsto W_{k+1}\) defined after this substitution, then \(\tilde\Phi_h = \tilde\Psi_h^{-1}\). Since \(P_{k+1/2}\leftrightarrow -P_{k+1/2}\) is exactly the same method under the above mentioned Hamiltonian substitution \(H\leftrightarrow -H\), we see that the method <strong>preserves the time-reversibility of the original system</strong>. Thus, in absence of root-finding and round-off errors, the method is time-reversible: if we take $n$ steps with the method, and from there apply another \(n\) steps with the same method but for the time-reversed Hamiltonian, then we then get back to the initial state.</p> <p>Even more structure is preserved by the method. Indeed, in our paper <d-cite key="MoVi2020a"></d-cite> we proved that \(\tilde\Phi_h\) is a Lie–Poisson map that evolves on the co-adjoint orbits. It means that the method itself defines a symplectic, discrete dynamical system on \(\mathrm{Orb}(W_0)\). Furthermore, one can, via <em>backward error analysis</em>, prove that this discrete dynamical system corresponds to the exact flow of a continuous Lie–Poisson system for a slightly modified Hamiltonian (cf. <d-cite key="HaLuWa2006"></d-cite>).<d-footnote>More precisely, this statement is true up to exponentially small terms.</d-footnote></p> <p>In summary, we see that the method \(\tilde\Phi_h\) itself is a time-reversible Lie–Poisson system and therefore cannot be dissipative or have an attractor. Thus, any numerical dissipation must originate from either root-finding or round-off errors. But how can we find out? The plot thickens.</p> <h2 id="second-experiment-there-and-back-again">Second experiment: there and back again</h2> <p>It is possible to give an estimate of how many time-steps are needed in order for root-finding and round-off errors to have a tangible effect on the dynamics. For a time-reversible method, as here, it can be directly tested by the following numerical experiment.</p> <ol> <li> <p>With initial conditions \(W_0\) as above, run a simulation forward for \(n\) time-steps, well beyond the formation of the blobs. This gives us the state \(W_n\).</p> </li> <li> <p>With initial conditions \(W_n\), re-start a new simulation, but now corresponding to the time-reversed system, and carry out \(n\) of time-steps. If the resulting state, let us call it \(W_{0\leftarrow n}\), is close to \(W_0\) (for example in max-norm \(\lVert W_{0\leftarrow n} - W_0\rVert_{\infty}\)), then any numerical dissipation caused by root-finding and round-off errors is neglectable in that time interval.</p> </li> </ol> <p>Below is the outcome of the experiment.</p> <div class="row justify-content-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/reversibility_vanishing_N_512_time_120_isofix_compsum.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <p>Voilà! The simulation returned to a state indistinguishable from the initial conditions. The key observation is that the four blobs emerge long before the stop and restart at \(W_n\). The conclusion is decisive: <strong>the mechanism for blob formations is not caused by numerical dissipation</strong>.</p> <p>Before I continue, let me give a few more details of the experiment. Even if the magnitude of each individual root-finding or round-off error is as the breadth of a hair to one astronomical unit, the accumulated error \(\lVert W_{0\leftarrow n} - W_0\rVert_{\infty}\) grows exponentially with \(n\). Thus, eventually these errors will reach the magnitude of \(\lVert W_0 \rVert_\infty\). Below is a diagram, in semi-log scale, displaying the relative error \(\lVert W_{0\leftarrow n} - W_0\rVert_{\infty}/\lVert W_0 \rVert_\infty\) for increasing \(n\).</p> <div class="row justify-content-center"> <div class="col-10 col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reversibility-error.svg" sizes="95vw"/> <img src="/assets/img/reversibility-error.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The first dashed line indicates when the mixing phase leading to the four blobs is over (the <em>mixing time</em>). The second dashed line indicates which \(n\) I used to produce the animation above. At \(n=7\cdot 10^5\), the relative error is about 1, so the accumulated error is as large as the initial data. After this point, \(n\) needs to be at least doubled again before the unwrapping of the blobs at \(W_{0\leftarrow n}\) has completely vanished.</p> <h2 id="statistics-in-zeitlins-model">Statistics in Zeitlin’s model</h2> <p>From the experiment just conducted, we conclude that mixing leading to blob formations occurs in Zeitlin’s model (to be precise, we showed it for the near-by Hamiltonian flow \(\tilde\Phi_h\)). Indeed, our previous rejection was too hasty: there are solutions that begin in a seemingly stable four blob configuration, then suddenly unwrap, only to return to a blob configuration again. But we never see generic solutions unwrap, for the same reason we never see the gas particles return to the first chamber: it is extraordinarily unlikely to happen within reasonable time intervals. And so the old paradox of Boltzmann and Poincaré appear again, but in the new shape of Zeitlin’s model.</p> <p>Here is a conjecture, intentionally vague in the spirit of V. Arnold (cf. <d-cite key="KhTa2014"></d-cite>):</p> <div class="fake-img l-body"> <p> For \(N\) large enough and \(W_0\in\mathfrak{su}(N)\) corresponding to smooth, generic vorticity, there exists a measure on \(\mathrm{Orb}(W_0)\), (close to) invariant under the Euler--Zeitlin equations \eqref{eq:zeitlin}, for which there is an overwhelming probability of blob-like configurations corresponding to large-scale integrable motion as described above. </p> </div> <p>The notion of using statistical mechanics in hydrodynamics is old: it was first suggested by Lars Onsager in an absolutely brilliant paper from 1949 <d-cite key="On1949"></d-cite>. Onsager considered, instead of gas particles, a square with a large but finite number of point vortices. These are weak solutions of the form \(\omega = \sum_{i=1}^N \Gamma_i \delta_{x_i}\). He then applied Boltzmann’s framework to the finite-dimensional dynamical system for the point-vortex positions \(x_i \in [0,1]\times[0,1]\), which preserves phase volume (owing to the Hamiltonian structure discovered already in 1858 by Helmholtz <d-cite key="He1858"></d-cite>). Our approach is similar to Onsager’s, only it is based on Zeitlin’s model instead of point-vortices. It can therefore address smooth vorticity fields, for which Casimirs are preserved (all the Casimirs break down for point-vortices, only energy survives as a first integral).</p> <p><strong>Remark</strong><br/> There are other approaches to adopt Onsager’s idea to smooth vorticity fields. Typically, these are based on some truncation that does not respect the conservation of Casimirs, except possibly enstrophy (cf. <d-cite key="BoEc2012"></d-cite><d-cite key="BoVe2012"></d-cite>). These theories, however, assume ergodicity and predict large-scale steady blob configurations. That is not what we see. Rather, near integrability of the large-scale blob interactions seem to block these predictions from happening, or at least significantly slow down the mixing, so it does not happen within feasible time-intervals.</p> <h2 id="summary">Summary</h2> <p>In this post I reviewed the classical paradox that came to light through Boltzmann’s H-theorem and Poincaré’s recurrence theorem. I demonstrated how blob formations in numerical simulations based on Zeitlin’s model, which at first sight appears to originate from numerical dissipation, actually turn out to display the same old paradox, but in a different setting.</p> <p>The question of a rigorous understanding of the generic long-time behavior of the 2-D Euler equations is a <em>jewel</em> of mathematical fluid dynamics. My long-term aspiration is that Zeitlin’s model, and numerical simulations based on it, shall prove important for further progress.</p>]]></content><author><name>Klas Modin</name></author><category term="graduate"/><category term="hydrodynamics"/><summary type="html"><![CDATA[a hydrodynamical version of the enigma by Boltzmann and Poincaré]]></summary></entry><entry><title type="html">Spectacular clouds over Mont Blanc</title><link href="https://klasmodin.github.io/blog/2023/les-houches/" rel="alternate" type="text/html" title="Spectacular clouds over Mont Blanc"/><published>2023-03-20T12:00:00+00:00</published><updated>2023-03-20T12:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2023/les-houches</id><content type="html" xml:base="https://klasmodin.github.io/blog/2023/les-houches/"><![CDATA[<div class="row justify-content-center"> <div class="col-12 col-sm-10"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/les_houches.apng" sizes="95vw"/> <img src="/assets/img/les_houches.apng" class="img-fluid" width="100%" height="auto" title="ice clouds" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Last week I attended a workshop on Optimal Transport Theory and Applications to Physics at <a href="https://www.houches-school-physics.com/en/">École de Physique des Houches</a>. The scenery at the school is absolutely magnificent, with stunning views to Mont Blanc. One late afternoon, after nearly a full week of interesting presentations and discussions, a few of us steped out of the lecture hall to energize ourselved in the crisp air against the backdrop of the setting sun. The moment became magical as we witnessed the formation of peculiar clouds above the mountain peaks. We all gazed in marvel as the cloud dynamics unfolded – much like some equation from the lecture hall but here in real sight.</p> <p>Standing next to me was <a href="https://www.metoffice.gov.uk/research/people/mike-cullen">Mike Cullen (UK Met Office)</a>, world-renowned expert on mathematical models in metrology. He told us that they are probably ice clouds (<a href="https://en.wikipedia.org/wiki/Cirrus_cloud">cirrus</a>). I took three photos, about five seconds between each (I wish I would have taken many more).</p> <p>As I came back home I stiched together a 3-frame <a href="https://en.wikipedia.org/wiki/APNG">animated PNG</a> of the photos (see image above). The dynamics of the cloud formations then really came through. <strong>Can you suggest a dynamical model with similar behaviour?</strong></p> <p>Here are some observations that I find interesting:</p> <ol> <li> <p>The large arc formation on the top is very sharp as it forms, but then undergoes diffusion.</p> </li> <li> <p>To the right of the large arc you see smaller ones forming, again initially very sharp.</p> </li> <li> <p>Sharp branching structures occur. Look at the first frame, just below the the large arc. You see four initially sharp branches that break up: the upper two are “captured” by the large upper arc, whereas the lower two are merged with the lower diffuse cloud to the right. In the last frame you see the region of the former branches separated by a void. The whole process reminds me of vortex dynamics in 2-D Euler equations, where smaller vortex condensates are sucked in by larger ones, except here there’s no swirling.</p> </li> <li> <p>There’s a peculiar “bubble” interaction going on in the lower left corner of the sky. (The dark patch in the first frame is probably a dust particle close to the lens.)</p> </li> </ol>]]></content><author><name></name></author><category term="popular"/><summary type="html"><![CDATA[dynamics of ice clouds seen from École de Physique des Houches in the French alps]]></summary></entry><entry><title type="html">What is shape analysis?</title><link href="https://klasmodin.github.io/blog/2023/what-is-shape-analysis/" rel="alternate" type="text/html" title="What is shape analysis?"/><published>2023-02-03T16:00:00+00:00</published><updated>2023-02-03T16:00:00+00:00</updated><id>https://klasmodin.github.io/blog/2023/what-is-shape-analysis</id><content type="html" xml:base="https://klasmodin.github.io/blog/2023/what-is-shape-analysis/"><![CDATA[<p>In school, we’re taught how Pythagoras’ theorem can be used to compute the distance between points in Euclidean space. The straight line segment connecting two points is the simplest example of a <em>geodesic curve</em> – an optimal path that connects one point with another. But how do you compute optimal warps that <em>deform geometric shapes</em> into one another? And what exactly does <em>optimal</em> mean? <d-footnote>This post is loosely based on <a href="https://slides.com/kmodin/what-is-shape-analysis">slides for a short presentation</a> I gave at Chalmers in 2020.</d-footnote></p> <p>We all have an intuition for similarity between shapes. Perhaps you agree that a right-angled triangle resembles an equilateral triangle more than a circle (or perhaps you don’t).</p> <div class="row justify-content-center"> <div class="col-12 col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/simple-shapes.svg" sizes="95vw"/> <img src="/assets/img/simple-shapes.svg" class="img-fluid" width="100%" height="auto" title="shapes" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Which two shapes are more alike? </div> <p>Whether so mathematically depends on the definition of <em>distance</em> between shapes. Indeed, in mathematics the concept of distance is abstract and goes well beyond Euclidean space. Thus, two shapes are similar if the distance between them is small. But we have to be careful: there’s no canonically given shape distance. What is suitable depends on the application. The mathematical theory of <strong>shape analysis</strong> is flexible enough to allow many choices, yet rigid enough to admit a unified framework for analysis and numerical computations.</p> <h2 id="historical-overview">Historical overview</h2> <p>The rich mathematical foundation of shape analysis springs from its history. The genesis can be found in painter and theorist Albrecht Dürer’s <a href="https://en.wikipedia.org/wiki/Albrecht_D%C3%BCrer#Four_Books_on_Human_Proportion">Vier Bücher von Menschlicher Proportion</a> from 1528, and, much later, in <a href="https://en.wikipedia.org/wiki/D%27Arcy_Wentworth_Thompson">D’Arcy Wentworth Thompson’s</a> influential 1917 book <em>On Growth and Form</em> <d-cite key="Th1917"></d-cite>. Thompson drew shapes of species of fish and related them by mathematical transformations. Later, from about 1970, <a href="https://en.wikipedia.org/wiki/Ulf_Grenander">Ulf Grenander</a> at Brown University was inspired by Thompson’s work and developed <a href="https://en.wikipedia.org/wiki/Pattern_theory">pattern theory</a>, with a model for continuous deformation mechanisms <d-cite key="Gr1993"></d-cite>. It comprises of a Lie group \(G\) acting on a metric space \(S\) of “shapes”. Deformations are then modeled as the group \(G\) acting on a fixed template shape \(s_0\in S\). In other words, deformations are of the form \(g\cdot s_0\) for \(g\in G\).</p> <div class="row justify-content-center"> <div class="col-12 col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://upload.wikimedia.org/wikipedia/commons/e/e9/Transformation_of_Argyropelecus_olfersi_into_Sternoptyx_diaphana-480.webp 480w,https://upload.wikimedia.org/wikipedia/commons/e/e9/Transformation_of_Argyropelecus_olfersi_into_Sternoptyx_diaphana-800.webp 800w,https://upload.wikimedia.org/wikipedia/commons/e/e9/Transformation_of_Argyropelecus_olfersi_into_Sternoptyx_diaphana-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="https://upload.wikimedia.org/wikipedia/commons/e/e9/Transformation_of_Argyropelecus_olfersi_into_Sternoptyx_diaphana.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="fish" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image drawn by D'Arcy Wentworth Thompson (Public domain, via Wikimedia Commons) </div> <p>During the 1990’s, together with biomechanical engineer <a href="https://en.wikipedia.org/wiki/Michael_I._Miller">Michael Miller</a> at John Hopkins University, Grenander applied the deformable template framework to variability in human anatomy. The need for such a model, and by extension the birth of <strong>computational anatomy</strong>, was a direct consequence of the increased availability of <a href="https://en.wikipedia.org/wiki/Magnetic_resonance_imaging">magnetic resonance imaging (MRI)</a> and other medical imaging techniques.</p> <p>The mathematical leap by Grenander and Miller was an infinite-dimensional setting, with \(G\) as a group of diffeomorphisms acting on a template 3-D image of, for example, the brain. The continued mathematical developments were influenced by a series of events.</p> <p>In the spring of 1997, Grenander and Miller presented their work in a symposium celebrating the 50th anniversary of the Division of Applied Mathematics at Brown. <a href="https://en.wikipedia.org/wiki/David_Mumford">David Mumford</a>, colleague and friend of Grenander, organized the symposium. Another breakthrough then came about. It was understood that computational anatomy is closely tied to <strong>topological hydrodynamics</strong> - the theory of geodesic equations on groups of diffeomorphisms, initiated by <a href="https://en.wikipedia.org/wiki/Vladimir_Arnold">Vladimir Arnold’s</a> 1966 discovery that the incompressible Euler equation is a (reduced) geodesic equation on volume preserving diffeomorphisms <d-cite key="Ar1966"></d-cite>. In one go, a large array of advanced mathematical theories was thereby enabled. Mumford went on to organize the 1998 trimester at <a href="https://www.ihp.fr/en">Institute Henri Poincaré</a> titled <em>Questions Mathématiques en Traitement du Signal et de l’Image</em>. In its proceedings, Mumford <d-cite key="Mu1998"></d-cite> derived, in Arnold-like fashion, the partial differential equation for geodesics on diffeomorphisms (we’ll return to this equation below). The meeting also fostered connections to global analysis and several rigorous results; notably those building on the work of <a href="https://atrouve.perso.math.cnrs.fr/">Alain Trouvé</a> at Ecole Normale Supérieure-Cachan on the metric structure of groups of diffeomorphisms induced by reproducing kernel Hilbert spaces <d-cite key="Tr1995"></d-cite>.</p> <p>By the year 2000, computational anatomy was thereby established at Brown, John Hopkins, and ENS-Cachan. Via <a href="http://www.cds.caltech.edu/~marsden/">Jerrold Marsden</a> at Caltech it also began to appear in contexts of geometric mechanics <d-cite key="HiMaAr2001"></d-cite>.</p> <p>As a next event, <a href="https://faculty.utah.edu/u0492366-SARANG_JOSHI/research/index.hml">Sarang Joshi</a>, student of Miller at the time, was working with a model where diffeomorphisms act on <em>landmark points</em>. Following a 2002 meeting on image analysis at Los Alamos National Laboratory, Miller discussed Joshi’s model with <a href="https://en.wikipedia.org/wiki/Darryl_Holm">Darryl Holm</a>, in particular its connection to soliton solutions in the <a href="https://en.wikipedia.org/wiki/Camassa%E2%80%93Holm_equation">Camassa-Holm shallow water equation</a>. Through Marsden, Holm, and many collaborators, computational anatomy then quickly spread also in the geometric mechanics community.</p> <p>From these and certainly also other events, the mathematical field of shape analysis took form.</p> <h2 id="warps-by-geodesics">Warps by geodesics</h2> <p>The starting point in shape analysis is a Riemannian manifold \(M\), usually compact. This is your spatial domain. To keep it simple, we continue here with \(M\) as the <a href="https://en.wikipedia.org/wiki/Hypercube">\(n\)-dimensional cube</a></p> \[M = \{ (x_1,\ldots,x_n)\mid x_k \in [0,1] \}\] <p>equipped with periodic boundary conditions for each coordinate. (It is called the <em>flat torus</em> in geometry and is usually though of as the quotient space \(\mathbb{R}^n/\mathbb{Z}^n\)).</p> <p>A <em>diffeomorphism</em> on \(M\) is a smooth, bijective map \(\varphi\colon M\to M\) whose inverse \(\varphi^{-1}\) is also smooth. Such a map deforms \(M\) but keeps its manifold structure intact. It’s helpful here to think of a uniform grid on \(M\) and then apply the map to each point on the gridlines to obtained a warped grid. If \(\varphi\) is a diffeomorphism, initially parallell gridlines cannot intersect or collide in the warped grid.</p> <div class="row justify-content-center"> <div class="col-10 col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/warp-example-bw-dark-480.webp 480w,/assets/img/warp-example-bw-dark-800.webp 800w,/assets/img/warp-example-bw-dark-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/warp-example-bw-dark.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="warp" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A diffeomorphisms can be visualized as the warp of a uniform grid. </div> <p>The warped grid is important in applications of shape analysis. We’ll return to it later, but first something about the mathematical structure of diffeomorphisms.</p> <h3 id="lie-group-and-lie-algebra">Lie group and Lie algebra</h3> <p>The set of all diffeomorphisms on \(M\) forms a group \(\operatorname{Diff}(M)\) with composition as group multiplication. Indeed, if \(\varphi\) and \(\eta\) are diffeomorphisms then \(\varphi\circ\eta\) is again a diffeomorphism. The group inversion is \(\varphi\mapsto\varphi^{-1}\). The group identity is the map \(\operatorname{id}\colon M\to M\) defined by \(\operatorname{id}(x) = x\). One should think of \(\operatorname{Diff}(M)\) as an infinite-dimensional Lie group. This notion is precise in the category of <em>Fréchet Lie groups</em> (cf. Kriegl and Michor <d-cite key="KrMi1997b"></d-cite>), but I won’t get into technical details in this post.</p> <p>Take now a smooth path \(\gamma: [0,1] \to \operatorname{Diff}(M)\) such that \(\gamma(0) = \operatorname{id}\). It is useful to picture \(\gamma\) as a <em>continuous</em> warp. Indeed, we can interpret \(\gamma\) as a map \(\gamma:[0,1]\times M \to M\). A point \(x\in M\) is then continuously moved along the path \([0,1]\ni t\mapsto \gamma(t,x) \in M\). If we differentiate this path we obtain a vector</p> \[\frac{\partial \gamma(t,x)}{\partial t} \in T_{\gamma(t,x)} M \simeq \mathbb R^n.\] <p>This vector depends smoothly on \(t\) and \(x\), but we can also think of it as a vector depending on \(t\) and the variable \(y \equiv \gamma(t,x)\). Since \(x = \gamma^{-1}(t,y)\) depends smoothly on \(y\), we thereby obtain a smooth map</p> \[(t,y) \mapsto v(t,y) \equiv \frac{\partial \gamma(\tau,\gamma^{-1}(t,y))}{\partial \tau}\Bigg|_{\tau=t} \in T_{y} M \simeq \mathbb R^n .\] <p>If we now instead fix \(t\), then \(v(t,\cdot) \equiv v_t\) maps a point \(y\in M\) to a vector in \(T_{y}M\). Thus, \(v_t\) is a smooth, time-dependent vector field on \(M\). By construction, the path \(t\mapsto \gamma(t,x)\) is a solution to the non-autonomous ordinary differential equation</p> \[\frac{\partial\gamma(t,x)}{\partial t} = v\big(t,\gamma(t,x)\big).\] <p>It can be written for all \(x\in M\) simultaneously if we again think of \(\gamma\) as a path on \(\operatorname{Diff}(M)\):</p> <p>\begin{equation}\label{eq:ode} \dot\gamma = v_t \circ \gamma, \quad \gamma(0) = \operatorname{id} \end{equation}</p> <p>where \(\dot\gamma\) denotes differentiation with respect to \(t\) (the notation originates from mechanics, where \(t\) is the time variable). Equation \eqref{eq:ode} captures a golden rule in shape analysis: warps are generated by integrating time-dependent vector fields.</p> <p>If we return to the interpretation of \(\operatorname{Diff}(M)\) as a Lie group, our derivations show that its Lie algebra is the space of smooth vector fields \(\mathfrak{X}(M)\). The associated bracket</p> \[[\cdot,\cdot]\colon \mathfrak{X}(M)\times \mathfrak{X}(M) \to \mathfrak{X}(M)\] <p>is the <a href="https://en.wikipedia.org/wiki/Lie_bracket_of_vector_fields">Jacobi-Lie bracket</a>, which in vector calculus notation, with \(u\) and \(v\) as column vectors, is given by</p> \[[v,u] = \nabla u \cdot v - \nabla v\cdot u\] <p>where “dot” denotes matrix multiplication and \(\nabla u\) is the elementwise gradient (geometrically it is the <a href="https://en.wikipedia.org/wiki/Covariant_derivative">co-variant derivative</a>).</p> <h3 id="riemannian-metric-on-the-group">Riemannian metric on the group</h3> <p>Our objective is to introduce a <a href="https://en.wikipedia.org/wiki/Metric_space">distance function</a> on \(G=\operatorname{Diff}(M)\). In other words, a function \(d_G\colon G\times G \to \mathbb{R}\) that for all \(\varphi, \eta,\psi\in G\) fulfills</p> <ul> <li><strong>vanishing distance to itself:</strong> \(d_G(\varphi,\varphi) = 0\)</li> <li><strong>positivity:</strong> if \(\varphi \neq \eta\) then \(d_G(\varphi,\eta)&gt; 0\)</li> <li><strong>symmetry:</strong> \(d_G(\varphi,\eta) = d_G(\eta,\varphi)\)</li> <li><strong>triangle inequality:</strong> \(d_G(\varphi,\psi) \leq d_G(\varphi,\eta) + d_G(\eta,\psi)\).</li> </ul> <p>In addition, we would like the distance to respect the group structure in the following sense</p> <ul> <li><strong>right invariance</strong>: \(d_G(\varphi,\eta) = d_G(\varphi\circ\psi,\eta\circ\psi)\).</li> </ul> <p>The last property is not an absolute requirement, but is natural to have. It implies that \(d_G(\operatorname{id},\varphi) = d_G(\operatorname{id},\varphi^{-1})\) and \(d_G(\operatorname{id},\varphi\circ\psi) \leq d_G(\operatorname{id},\varphi) + d_G(\operatorname{id},\psi)\). <d-footnote>One can also achieve these properties via left invariance. It turns out, however, that right invariance is favourable in the existence and uniqueness analysis (which itself is a fascinating story).</d-footnote></p> <p>Once you have a distance function, you can measure the “length” of \(\varphi\in\operatorname{Diff}(M)\) as its distance to the identity \(d_G(\operatorname{id},\varphi)\). But we want more than that. We want, in addition, a <em>continuous</em> warp from \(\operatorname{id}\) to \(\varphi\). In other words, a smooth path \(\gamma\colon [0,1] \to \operatorname{Diff}(M)\) with \(\gamma(0)=\operatorname{id}\) and \(\gamma(1) = \varphi\). There is a way to do both things simultaneously and it spells <a href="https://en.wikipedia.org/wiki/Riemannian_geometry">Riemannian geometry</a>. The notion goes back to <a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss">Carl Friedrich Gauss</a> and <a href="https://en.wikipedia.org/wiki/Bernhard_Riemann">Bernhard Riemann</a> and is today a major branch of geometry. Here is chiefly how it works:</p> <p>A manifold \(Q\) admits tangent spaces and for each \(q\in Q\) its tangent space \(T_qQ\) is linear. If we assign to each tangent space an inner product \(\langle\cdot,\cdot\rangle_q\), then we can measure the length of a \(C^1\) curve segment \(\gamma\) in \(Q\) as</p> <p>\begin{equation}\label{eq:length} \ell = \int_{t_0}^{t_1} \sqrt{\langle\dot\gamma(t),\dot\gamma(t)\rangle_{\gamma(t)}}\, dt \end{equation}</p> <p>The length is independent of how we parameterize \(\gamma\) (prove this!), so it truly gives a length to \(\gamma\) as a <em>curve</em>. To obtain a distance function \(d_Q(q_0,q_1)\), find among all curve segments connecting \(q_0\) and \(q_1\) one that minimizes the length. This length is the distance and the corresponding curve segment is called a <strong>geodesic curve</strong> between \(q_0\) and \(q_1\). The distance function defined this way will automatically fulfill the four required properties listed above.</p> <p>As you guessed, the aim is to equip \(\operatorname{Diff}(M)\) with a Riemannian metric \(\langle\cdot,\cdot\rangle_{\varphi}\) and thereby obtain a distance function \(d_G\). There’s still some work to do, however, because of the additional property of right invariance. Also, how does this Riemannian story connect to the golden rule in \eqref{eq:ode}? Mathematically we’re in the intersection between Lie theory and Riemannian geometry, which is overall an important theme is geometric mechanics with many interesting examples: rigid body motion, incompressible fluids, Heisenberg spin chain, Korteweg–De Vries (KdV) equation, Camassa-Holm equation, magneto-hydrodynamics, etc. (cf. Arnold and Khesin <d-cite key="ArKh1998"></d-cite>).</p> <p>Right invariance of \(d_G\) in the Riemannian case implies that the length of \(\gamma(t)\) must be the same as the length of \(\tilde\gamma(t) \equiv \gamma(t)\circ\eta\) for any \(\eta\in G\). Since \(\dot{\tilde\gamma}(t) = \dot\gamma(t)\circ\eta\), we conclude from the length formula \eqref{eq:length} that the Riemannian metric must fulfill</p> \[\langle\dot\gamma(t),\dot\gamma(t)\rangle_{\gamma(t)} = \langle\dot\gamma(t)\circ\eta,\dot\gamma(t)\circ\eta\rangle_{\gamma(t)\circ\eta}\] <p>This formula relates the inner product at \(T_{\gamma(t)}G\) to the inner product at \(T_{\gamma(t)\circ\eta}G\). In fact, one is determined from the other. Since this should hold for any \(\eta\in G\), and since all elements in \(G\) can be reached by suitably selecting \(\eta\), it follows that a right invariant Riemannian metric is fully determined from only one inner product. Naturally, as the defining inner product for the right invariant Riemannian metric, we take the one at the identity tangent space \(T_{\operatorname{id}}G\), i.e., at the Lie algebra of \(G\). The formula for the length of \(\gamma\) then becomes</p> \[\ell = \int_{t_0}^{t_1} \sqrt{\langle\dot\gamma(t)\circ\gamma(t)^{-1},\dot\gamma(t)\circ\gamma(t)^{-1}\rangle_{\operatorname{id}}}\, dt\] <p>Does this look familiar? The quantity \(\dot\gamma(t)\circ\gamma(t)^{-1}\) is the vector field \(v_t\) in the golden rule equation \eqref{eq:ode} above! Expressed differently, if \(\gamma(t)\) is generated by a time-dependent vector field \(v_t\) as in equation \eqref{eq:ode}, then the length of \(\gamma(t)\) with respect to a right invariant Riemannian metric is given by</p> \[\ell(v) = \int_{t_0}^{t_1} \sqrt{\langle v_t,v_t \rangle_{\operatorname{id}}}\, dt .\] <p>A geodesic curve between \(\operatorname{id}\) and \(\varphi\) is thereby obtained by minimizing \(\ell(v_t)\) under the constraint that \(\gamma(t)\) generated by \(v_t\) according to \eqref{eq:ode} should fulfill \(\gamma(1)=\varphi\).</p> <p>The only thing left to worry about is how to choose the inner product \(\langle \cdot,\cdot\rangle_{\operatorname{id}}\) on the Lie algebra \(\mathfrak{X}(M)\). As I already indicated, there is no canonical choice that always works: the choice is part of the particular shape model one is interested in. There is, however, one choice that at first seems natural, but which is not good: the \(L^2\) inner product on \(\mathfrak{X}(M)\). This choice doesn’t give a well-posed geodesic equation: infinite-dimensional complications pop up in the analysis since the corresponding norm is not strong enough. The standard choice is instead to use a higher-order Sobolev inner product, for example the \(H^k\) inner product</p> \[\langle v,v\rangle_{\operatorname{id}} = \int_M v (1-\Delta)^k v \, dx.\] <p>For our purposes, it is convenient to keep things general and just assume that the inner product is of the form \(\langle v,v\rangle_{\operatorname{id}} = \langle v, Av\rangle_{L^2}\) for some self-adjoint, positive operator \(A\) (often referred to as an <em>inertia operator</em>, again borrowing from the language of geometric mechanics).</p> <p>We now have everything we need to formulate the geodesic problem:</p> \[\min_{v\colon [0,1]\to \mathfrak{X}(M)} \int_0^1 \sqrt{\langle v_t, Av_t\rangle_{L^2}}\, dt\] <p>under the constraints</p> \[\dot\gamma(t) = v_t\circ\gamma(t), \quad \gamma(0)=\operatorname{id}, \quad \gamma(1) = \varphi .\] <p>Once \(v\) is found, the distance is given by</p> \[d_G(\operatorname{id},\varphi) = \ell(v).\] <p>(Since the distance is right invariant, it is enough to define it from the identity).</p> <h3 id="connection-to-kinetic-energy-systems">Connection to kinetic energy systems</h3> <p>The length functional \eqref{eq:length} in Riemannian geometry is a little bit cumbersome to work with. Partly because it contains a square root, and partly because it doesn’t provide a unique solution among parameterized curves \(\gamma(t)\) (any reparameterization gives another solution, remember). There is a beautiful resolution to this problem, which connects Riemannian geodesics to kinetic energy systems in mechanics. Such a system is defined by a Riemannian metric \(\langle \cdot,\cdot\rangle_{q}\) on a <em>configuration manifold</em> \(Q\). It describes a particle moving on \(Q\) with respect to the Lagrangian given by the Riemannian metric \(L(q,\dot q) = \frac{1}{2}\langle \dot q,\dot q\rangle_q\). The variational principle of mechanics states that \(\gamma(t)\) is a solution curve if, among all curves with fixed end-points \(\gamma(t_0)\) and \(\gamma(t_1)\), it extremizes the action functional</p> \[S(\gamma) = \int_{t_0}^{t_1} L\big(\gamma(t),\dot \gamma(t)\big) \, dt\] \[= \frac{1}{2} \int_{t_0}^{t_1} \langle \dot\gamma(t),\dot\gamma(t)\rangle_{\gamma(t)}\, dt .\] <p><strong>Lemma:</strong> <br/> \(\gamma(t)\) is a solution curve to the mechanical system if and only if it is a geodesic curve (with respect to \(\langle \cdot,\cdot\rangle_{q})\), parameterized so that \(\frac{d}{dt}\langle\dot\gamma(t),\dot\gamma(t)\rangle_{\gamma(t)} = 0\).</p> <p><strong>Proof sketch:</strong> <br/> The key is that if \(\gamma(t)\) is a mechanical solution curve, then the <em>kinetic energy</em> \(T(q,\dot q) = \frac{1}{2}\langle \dot q,\dot q\rangle_{q}\) is a first integral (prove this!). Take now a variation of \(\gamma(t)\), i.e., a map \((t,\epsilon)\to \tilde\gamma(t,\epsilon)\in Q\) such that \(\tilde\gamma(t,0) = \gamma(t)\). We first show that</p> \[\frac{d}{d\epsilon}\Bigg|_{\epsilon=0} S(\tilde\gamma_\epsilon) = 0 \quad\Rightarrow\quad \frac{d}{d\epsilon}\Bigg|_{\epsilon=0} \ell(\tilde\gamma_\epsilon) = 0 .\] <p>Let’s calculate the right-most variation</p> \[\frac{d}{d\epsilon}\Bigg|_{\epsilon=0} \ell(\tilde\gamma_\epsilon) = \int_{t_0}^{t_1} \frac{1}{2}\frac{\frac{d}{d\epsilon}\langle \dot{\tilde\gamma}(t,\epsilon),\dot{\tilde\gamma}(t,\epsilon)\rangle_{\tilde\gamma(t,\epsilon)} }{\sqrt{\langle \dot\gamma(t),\dot\gamma(t)\rangle_{\gamma(t)}}}\] \[= \frac{t_1-t_0}{2 \sqrt{T(\gamma(0),\dot\gamma(0))}} \frac{d}{d\epsilon}\Bigg|_{\epsilon=0} S(\tilde\gamma_\epsilon)\] <p>where in the last equality we use that \(T\) is conserved. Since \(\dot\gamma(0) \neq 0\) we get that \(T(\gamma(0),\dot\gamma(0)) &gt; 0\) and so the result follows.</p> <p>For the other direction, let \(\gamma(t)\) be a geodesic curve parameterized so that it has constant speed. This means exactly that \(T\) is conserved along \(\gamma(t)\). We now simply reverse the calculation above. <em>QED.</em></p> <p>Instead of solving the original geodesic minimization problem, we can now solve the corresponding kinetic energy variational problem, which is simpler (no square root) and also doesn’t have the re-parameterization degeneracy. Furthermore, once we’ve computed the curve \(\gamma(t)\) this way, its length is given simply by \(\ell(\gamma) = (t_1-t_0)\lVert \dot\gamma(0)\rVert_{\gamma(0)}\), where \(\lVert\cdot\rVert_q\) is the norm corresponding to \(\langle\cdot,\cdot\rangle_q\).</p> <p>The geodesic–kinetic energy equivalence carries over to the right invariant geodesic problem on \(\operatorname{Diff}(M)\) formulated in terms of the vector field \(v_t\). Indeed, it can be reformulated as</p> \[\min_{v\colon [0,1]\to \mathfrak{X}(M)} \frac{1}{2}\int_0^1\langle v_t, Av_t\rangle_{L^2}\, dt\] <p>under the constraints</p> \[\dot\gamma(t) = v_t\circ\gamma(t), \quad \gamma(0)=\operatorname{id}, \quad \gamma(1) = \varphi .\] <p>The functional to be minimized is now quadratic in \(v\) and the distance function is given by</p> <p>\begin{equation}\label{eq:energy_from_initial} d_G(\operatorname{id},\varphi) = \sqrt{\langle v_0,A v_0 \rangle_{L^2}} . \end{equation}</p> <h2 id="geodesic-shape-matching">Geodesic shape matching</h2> <p>So far we’ve learned quite a bit about geodesics on diffeomorphisms, but little about shapes. First of all, what are shapes?</p> <p>To answer this question we need to talk about <em>group actions</em>. A Lie group \(G\) is said to have a <em>left action</em> on another manifold \(S\) if there is a map \(\varrho\colon G\times S\to S\) that for any \(\varphi,\eta\in G\) and \(s\in S\) preserves left group multiplication</p> \[\varrho(\eta,\varrho(\varphi, s)) = \varrho(\eta\circ\varphi, s) .\] <p>Usually the action map \(\varrho\) is left out in the notation and replaced by \(\varphi\cdot s \equiv \varrho(\varphi,s)\). The condition then looks almost like an <a href="https://en.wikipedia.org/wiki/Associative_property">associative law</a></p> \[\eta\cdot (\varphi \cdot s) = (\eta\circ\varphi)\cdot s .\] <p>Now, in shape analysis, a <em>shape space</em> is any manifold \(S\), finite or infinite dimensional, upon which the group \(G=\operatorname{Diff}(M)\) acts. Here are some common examples:</p> <ul> <li> <p>Smooth functions \(C^\infty(M,\Omega)\) for some co-domain \(\Omega\subset \mathbb{R}^d\). The action is \(\varphi\cdot f = f\circ\varphi^{-1}\). Real valued functions with \(\Omega = [0,1]\) are usually thought of as “pre-discretized” gray-scale images. Color images (with red, green, blue channels) correspond to \(\Omega = [0,1]^3\).</p> </li> <li> <p><em>Landmark space</em> is \(M^d\), i.e., a tuple of \(d\) points in \(M\). The actions is \(\varphi\cdot (p_1,\ldots,p_d) = (\varphi^{-1}(p_1),\ldots, \varphi^{-1}(p_d))\). Normally it is required that all points \(p_1,\ldots,p_d\) are different from each other.</p> </li> <li> <p>Immersions \(\mathbb{S}^1\to M\), i.e., closed, smooth curves \(\sigma(s)\) in \(M\). The action is \(\varphi\cdot \sigma = \varphi^{-1}\circ\sigma\).</p> </li> <li> <p>Smooth <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density functions (PDFs)</a> on \(M\). These are smooth, strictly positive functions \(C^\infty(M,\mathbb{R}_{&gt;0})\) that fulfill \(\int_M \rho \, dx = 1\). The action is \(\varphi\cdot\rho = \rho\circ\varphi^{-1} \lvert D\varphi^{-1}\rvert\). Notice that this action preserves the normalization of \(\rho\) (prove this!).</p> </li> </ul> <h3 id="warping-templates">Warping templates</h3> <p>Let now \(S\) be a shape space for \(G=\operatorname{Diff}(M)\). The idea of Grenander is to start from a <em>template shape</em> \(s_0 \in S\) and then deform it by applying a diffeomorphic warp \(\gamma(t)\). We then obtain a corresponding warp \(t\mapsto \gamma(t)\cdot s_0\) in \(S\). One can do all sorts of interesting things with such warps (why not random walks in shape space!). I’ll focus here on the <em>matching problem</em>, where there’s also a <em>target shape</em> \(s_1\in S\), and the objective is to find a warp \(\gamma(t)\) that deforms \(s_0\) to \(s_1\). Actually, this is not quite true; to require \(\gamma(1)\cdot s_0 = s_1\) is too rigid, for two reasons.</p> <ol> <li> <p>Typically not all elements in \(S\) can be reached by action on \(s_0\); only those on the <em>orbit</em> \(G\cdot s_0\) of \(s_0\). For example, if \(s_0 \in C^\infty(M,\mathbb{R})\) is generic (it’s a <a href="https://en.wikipedia.org/wiki/Morse_theory">Morse function</a>), then \(\varphi\cdot s_0\) preserves the number of <a href="https://en.wikipedia.org/wiki/Critical_point_(mathematics)#Several_variables">critical points</a> and their values: if \(x_c\) is a critical point of \(s_0\), then \(\varphi^{-1}(x_c)\) is a critical point of \(\varphi\cdot s_0\). Thus, a necessary requirement for \(s_1\) to belong to the same orbit as \(s_0\) is that it has the same number of critical points and the same set of critical values (<em>cf.</em> <a href="https://en.wikipedia.org/wiki/Morse_homology">Morse homology</a>). This is essentially never the case in applications. <d-footnote>There is actually a situation where the orbit consists of the entire shape space: probability density functions. Indeed, this is a result of Moser <d-cite key="Mo1965"></d-cite>. The story of shape analysis in this case is extremely interesting: it leads to optimal mass transport. But that story is for another post.</d-footnote></p> </li> <li> <p>It is usually not desirable to get as “close as possible” to the target \(s_1\), since it would lead to an extremely complicated warp that is hard to resolve numerically. The aim is a nice enough warp that takes the template sufficiently close to the target.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-6 col-sm-5"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hand_source-480.webp 480w,/assets/img/hand_source-800.webp 800w,/assets/img/hand_source-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hand_source.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="template image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-6 col-sm-5"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hand_target-480.webp 480w,/assets/img/hand_target-800.webp 800w,/assets/img/hand_target-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hand_target.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="target image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Example from medical imaging of a template (left) and a target (right). </div> <p>Of course, “nice enough” and “sufficiently close” are not very rigorous statements. Let’s make mathematical sense of it.</p> <p>We first introduce a distance function \(d_S\) on \(S\). This allows us to measure how close \(\varphi\cdot s_0\) is to the target \(s_1\). A typical example for functions, i.e., \(S=C^\infty(M,\mathbb{R})\), is to take the \(L^2\) norm.</p> <p>Next, recall from above that we already have the Riemannian distance function \(d_G\) on the group. We then require the distance \(d_G(\operatorname{id},\varphi)\) to be not too large (this is the “niceness” of the warp). The basic matching problem in shape analysis is the following:</p> <div class="fake-img l-body"> <p> Find \(\varphi\in G\) that minimizes </p> <p> $$ d_G(\operatorname{id},\varphi)^2 + \frac{1}{\sigma^2}d_S(\varphi\cdot s_0, s_1)^2 $$ </p> <p> where \(\sigma &gt; 0\) is a parameter that balances <i>regularity</i> of the warp (first term) against <i>similarity</i> between the shapes (second term). </p> </div> <p>Now we take a leap by incorporating everything we’ve learned above about the Riemannian distance \(d_G\). Recall the golden rule: we want to generate diffeomorphisms via time-dependent vector fields. Thus, we reformulate the problem instead as minimization over \(v_t\), which gives the following:</p> <div class="fake-img l-body"> <p> Find \(v\colon [0,1]\to \mathfrak{X}(M)\) that minimizes </p> <p> \begin{equation}\label{eq:shape_energy} E(v) = \int_0^1 \langle v_t,A v_t\rangle_{L^2} \, dt + \frac{1}{\sigma^2} d_S(\gamma(1)\cdot s_0, s_1)^2 \end{equation} </p> <p> where \(\gamma(1)\) is determined from \(v\) via </p> <p> \begin{equation}\label{eq:reconstruction} \dot\gamma(t) = v_t\circ\gamma(t), \quad \gamma(0) = \operatorname{id} . \end{equation} </p> </div> <p>This is the <strong>geodesic shape matching problem</strong>.<d-footnote>In the litterature, this problem is often called <i>LDDMM</i>. It's a tongue-twisting acronym, which stands for different things in different papers. I prefer <i>geodesic shape matching</i> – because that's what it is!</d-footnote></p> <h3 id="governing-partial-differential-equations">Governing partial differential equations</h3> <p>Thus far we have formulated the basic matching problem in shape analysis, but we have not yet solved it. More precisely, to “solve it” means two things. First, to solve the mathematical problem, i.e., prove that it has a solution. Second, to find an algorithm that can be used to compute a numerical (approximative) solution. Of course, the two go hand-in-hand and are both essential parts of shape analysis.</p> <p>Existence (but not uniqueness) of solutions to the geodesic shape matching problem can be established by a variant of the <a href="https://en.wikipedia.org/wiki/Direct_method_in_the_calculus_of_variations">direct method in calculus of variations</a>. There are quite a few technical details in the proof, so I’m not going to repeat it here. An excellent exposition is given in the monograph on shape analysis by Younes <d-cite key="Yo2010"></d-cite>. The proof yields existence of a minimizer, but not uniqueness. In fact, one cannot expect uniqueness: it is easy to construct a setup within the framework where the solution is not unique.</p> <p>An alternative to the direct method is to work out the governing differential equations via the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation">Euler-Lagrange equations</a> and then try to analyze those equations. The first observation is that \(\gamma(t)\) must be a geodesic curve on \(G\). Why? Because the second term in the matching energy \eqref{eq:shape_energy} depends only on the end-point \(\gamma(1)\). Consequently, if \(\gamma(t)\) is a curve that extremizes the energy functional \(E\) for variations that vanish only at the initial point \(\gamma(0)=\operatorname{id}\), it must also extremize the functional for variations that vanish both at the initial and end points. Viewed differently, the geodesic shape matching problem consists of finding a minimizing geodesic \(\gamma(t)\) on \(G\) with \(\gamma(0)=\operatorname{id}\) and initial velocity \(\dot\gamma(0)\) chosen so that \(d_S(\gamma(t)\cdot s_0, s_1)\) is minimized (it’s a <em>shooting problem</em>).</p> <p>To obtain the geodesic equation on \(G\), we first need to understand how variations of \(\gamma\) propagate to variations in \(v\). For a variation \(\tilde\gamma_\epsilon\) of \(\gamma\) we have a corresponding variation \(\tilde v_{\epsilon}\) of \(v\), defined by</p> \[\tilde v_{\epsilon}\circ \tilde\gamma_\epsilon = \dot{\tilde\gamma}_\epsilon .\] <p>If we differentiate this relation with respect to \(\epsilon\) we obtain, using the chain rule, that</p> \[(\delta\tilde v_{\epsilon} ) \circ \gamma + (\nabla v)\circ \gamma \cdot \delta\tilde\gamma_\epsilon = \delta\dot{\tilde\gamma}_\epsilon\] <p>where \(\delta\) denotes differentiation with respect to \(\epsilon\) at \(\epsilon = 0\), i.e., \(\delta = \frac{d}{d\epsilon}\big|_{\epsilon=0}\). Thus, \(\tilde v_\epsilon\) fulfills</p> <p>\begin{equation}\label{eq:v-variation} \delta\tilde v_\epsilon = \delta\dot{\tilde\gamma} \circ \gamma^{-1} - \nabla v\cdot (\delta\tilde\gamma_\epsilon\circ\gamma^{-1}) . \end{equation}</p> <p>The quantity \(u \equiv \delta\tilde\gamma_\epsilon\circ\gamma^{-1}\) is a mapping \((t,x)\mapsto u(t,x) \in T_x M\). In other words, \(u\) is a time-dependent vector field on \(M\). From its definition we see that its time derivative fulfills</p> \[\delta\dot{\tilde\gamma}_\epsilon = \dot u \circ\gamma + (\nabla u)\circ\gamma \cdot \dot\gamma .\] <p>Compose this expression with \(\gamma^{-1}\) and plug-in the result in \eqref{eq:v-variation} to obtain</p> <p>\begin{equation}\label{eq:v-variation-final} \delta\tilde v_\epsilon = \dot u + \nabla u\cdot v - \nabla v\cdot u . \end{equation}</p> <p>It’s getting interesting! The pairing of \(\nabla v\) with \(u\) is exactly the co-variant derivative of \(v\) along \(u\) (and vice versa), from now on denoted \(\nabla_u v \equiv \nabla v\cdot u\). So the last two terms in \eqref{eq:v-variation-final} constitute the Jacobi-Lie bracket, which, as we saw above, is the Lie bracket for the Lie algebra \(\mathfrak{X}(M)\) of \(\operatorname{Diff}(M)\). That variations of \(v\) take this form is certainly not a coincidence: it reflect the general form of variations in <em>Euler-Poincaré equations</em> (cf. Marsden and Ratiu <d-cite key="MaRa1999"></d-cite>).</p> <p>We are now ready to compute the variation of the action functional</p> \[S(\gamma) = \frac{1}{2}\int_0^1 \int_M v \cdot Av \, dx \, dt\] <p>for geodesics on \(G\). Indeed,</p> \[\delta S(\tilde \gamma_\epsilon) = \int_0^1 \int_M \delta \tilde{v}_{\epsilon,t} \cdot Av \, dx \, dt\] <p>which combined with \eqref{eq:v-variation-final} yields</p> \[\delta S(\tilde \gamma_\epsilon) = \int_0^1 \int_M (\dot u + \nabla_v u - \nabla_u v) \cdot Av \, dx \, dt .\] <p>Now we need to isolate \(u\) to get (think of \(u\) as the generator for the variation of \(\gamma\)). Integration by parts is our friend here. First, with respect to time, using that \(u(0,\cdot) = u(1,\cdot) = 0\), so the boundary terms vanish</p> \[\delta S(\tilde \gamma_\epsilon) = \int_0^1 \int_M \Big[ (\nabla_v u - \nabla_u v) \cdot Av - u\cdot \frac{d}{dt}Av \Big] dx \, dt\] <p>which can also be written</p> \[\delta S(\tilde \gamma_\epsilon) = \int_0^1 \Big[ \langle \nabla_v u - \nabla_u v, Av \rangle_{L^2} - \langle u , \frac{d}{dt}Av \rangle_{L^2} \Big] dt.\] <p>Next, we need to compute the adjoint of the linear operators \(u\mapsto \nabla_v u\) and \(u\mapsto \nabla_u v\). It takes some effort on a general manifold, but in our case, for the flat torus \(M=\mathbb{R}^n/\mathbb{Z}^n\), the coordinate variables \(x^i\) can be handled independently which makes it much easier. For the first operator</p> \[\langle \nabla_v u, w \rangle_{L^2} = \int_{M} \sum_{i=1}^n \sum_{j=1}^n \frac{\partial u^i}{\partial x^j} v^j w^i = \int_{M} \sum_{i=1}^n \sum_{j=1}^n u^i \frac{\partial}{\partial x^j}(-v^j w^i)\] \[= \int_{M} \sum_{i=1}^n \sum_{j=1}^n u^i \Big(-\frac{\partial v^j}{\partial x^j} w^i - v^j\frac{\partial w^i}{\partial x^j} \Big) = \langle u, -\nabla_v w - \operatorname{div}(v)w\rangle_{L^2}\] <p>Thus, the adjoint is given by \(-\nabla_v w - \operatorname{div}(v)w\). (This formula is valid also for a general manifold). The second operator \(u\mapsto \nabla_u v\) doesn’t take any derivatives of \(u\), so computing its adjoint is more a question of index bookkeeping:</p> \[\langle \nabla_u v, w \rangle_{L^2} = \int_{M} \sum_{i=1}^n \sum_{j=1}^n \frac{\partial v^i}{\partial x^j} u^j w^i\] \[= \sum_{j=1}^n \sum_{i=1}^n u^j \frac{\partial v^i}{\partial x^j} w^i = \langle u, \nabla v^\top w \rangle_{L^2} .\] <p>Using these results for the adjoints we obtain</p> \[\delta S(\tilde \gamma_\epsilon) = \int_0^1 \langle u, -\nabla_v Av - \operatorname{div}(v)Av - \nabla v^\top Av - \frac{d}{dt}Av \rangle_{L^2} \, dt.\] <p>The curve \(\gamma(t)\) is a geodesic if and only if \(\delta S(\tilde\gamma_\epsilon)=0\) for all admissible variations, i.e., for all time-dependent vector fields \(u\) vanishing at the end points. As seen in the last calculation, the necessary and sufficient condition is that the variable \(m\equiv Av\) (called <em>momentum</em> in mechanics) fulfills the following partial differential equation</p> <p>\begin{equation} \label{eq:epdiff} \dot m + \nabla_v m + \operatorname{div}(v)m + \nabla v^\top m = 0, \quad m =A v \, . \end{equation}</p> <p>This equation is the one Mumford derived in the proceedings mentioned above <d-cite key="Mu1998"></d-cite>. Today we call it the <em>EPDiff equation</em>, which is short for “Euler-Poincaré equation for the group of diffeomorphisms”.</p> <p>We are now ready to give the “shooting formulation” of the geodesic shape matching problem:</p> <div class="fake-img l-body"> <p> Find initial conditions \(m_0\in \mathfrak{X}(M)\) for equation \eqref{eq:epdiff} that minimize the energy </p> <p> \begin{equation}\label{eq:shooting_form} E(m_0) = \langle m_0 , A^{-1}m_0 \rangle + d_S(\gamma(1)\cdot s_0, s_1)^2 \end{equation} </p> </div> <p>That the first term in \(E(m_0)\) is so simple follows from equation \eqref{eq:energy_from_initial} above. The second term, on the other hand, is complicated. Indeed, to evaluate it we need to</p> <ol> <li> <p>solve the EPDiff equation \eqref{eq:epdiff} for \(t\in [0,1]\) with initial conditions \(m_0\);</p> </li> <li> <p>solve equation \eqref{eq:reconstruction} to obtain \(\gamma(t)\);</p> </li> <li> <p>then, finally, compute the distance \(d_S(\gamma(1)\cdot s_0, s_1)^2\).</p> </li> </ol> <h3 id="numerical-discretization-rough-sketch">Numerical discretization (rough sketch)</h3> <p>I’ve given you (more or less) a complete mathematical description of shape analysis in its most simple setting. To be useful in applications, however, you also need to discretize the equations so that they can be used in the computer on real data. The theory of how to numerically discretize differential equations is a <strong>huge</strong> field – much larger than shape analysis. I’m not going to give a detailed account of how we discretize the geodesic shape matching problem. On the larger scale, however, there are two different approaches, and these two exactly match the two different types of analysis just presented: the direct method and the shooting formulation.</p> <p>The formulation in equation \eqref{eq:shape_energy}, pertinent to the direct method in calculus of variations, suggests the following numerical method:</p> <p>Replace the time-dependent vector field \(v_t\) with a sequence of vector fields \(v_1,\ldots,v_k\) and replace the energy functional \eqref{eq:shape_energy} by the corresponding Riemann sum</p> \[E(v_1,\ldots, v_k) = \frac{1}{k}\sum_{i=1}^k \langle v_i,A v_i \rangle_{L^2} + \frac{1}{\sigma^2} d_S(\gamma_k \cdot s_0, s_1)^2\] <p>where \(\gamma_k\cdot s_0\) is obtained from \(v_1,\ldots,v_k\) by some <a href="https://en.wikipedia.org/wiki/Numerical_integration">numerical integration algorithm</a> (for example a Runge-Kutta method). Start with \(v_i^{(0)} = 0\), then minimize the energy functional \(E(v_1,\ldots,v_k)\) with the <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient decent method</a>:</p> \[v_i^{(j+1)} = v_i^{(j)} - \frac{\partial E}{\partial v_i}(v_1^{(j)},\ldots, v_k^{(j)})\] <p>Of course, this also requires a spatial discretization of the space of vector fields, for example using the <a href="https://en.wikipedia.org/wiki/Finite_difference_method">finite difference method</a> or the <a href="https://en.wikipedia.org/wiki/Finite_element_method">finite element method</a>.</p> <p>The other numerical approach is based on the shooting formulation in equation \eqref{eq:shooting_form}. It requires a numerical time-stepping algorithm for solving the EPDiff equation \eqref{eq:epdiff} as an initial value problem. Thus, each initial momentum \(m_0\) gives rise to a corresponding discretized path \(\gamma_1,\ldots,\gamma_k\). This is where the warped mesh from above come back into the picure. Indeed, the way we discretize a diffeomorphism is to think of it as a deformed mesh, whose nodes move (in discrete time) according to the vector fields \(v_1,\ldots,v_k\). If the template \(s_0\) is a function, we can now evaluate the deformed function \(s_0\circ\gamma_k^{-1}\) at the regular mesh nodes by evaluating \(s_0\) at the corresponding deformed mesh nodes. To obtain the optimal, we again use the gradient descent method, but now over the (discretized) momentum variable \(m_0\) for the energy functional \(E(m_0)\) in equation \eqref{eq:shooting_form}.</p> <div class="row justify-content-center"> <div class="col-12 col-sm-10"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hand-warp-1-480.webp 480w,/assets/img/hand-warp-1-800.webp 800w,/assets/img/hand-warp-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hand-warp-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hand warp 1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-12 col-sm-10"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hand-warp-2-480.webp 480w,/assets/img/hand-warp-2-800.webp 800w,/assets/img/hand-warp-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hand-warp-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hand warp 1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An example of solving the matching problem numerically <d-cite key="BaJoMo2015"></d-cite>. </div> <h2 id="summary">Summary</h2> <p>We’ve seen in this post how the mathematical theory of shape analysis came about and how it looks like in the simplest case. There is, of course, a lot more to know about this field. Look out for future posts! :smiley:</p> <p>In the meantime, the papers by Beg <em>et al</em> <d-cite key="BeMiTrYo2005"></d-cite> and by Bruveris and Holm <d-cite key="BrHo2013"></d-cite> are very good sources for a more detailed study. The “bible” in the field is the monograph by Younes titled <em>Shapes and Diffeomorphisms</em> <d-cite key="Yo2010"></d-cite>.</p> <p>There is also a different viewpoint on shape analysis, which directly looks at infinite-dimensional manifolds of embedded submanifolds without going through diffeomorphisms; see <a href="https://www.dam.brown.edu/people/mumford/vision/papers/2009b-12c--PisaReport-preprint.pdf">these lecture notes by Mumford</a> or, for more details and comparison between the two approaches, see the papers by Micheli <em>et al</em> <d-cite key="MiMiMu2013"></d-cite> and Bauer <em>et al</em> <d-cite key="BaBrMi2014"></d-cite>.</p> <p>I should also stress that what you’ve seen in this post only reflect the single matching problem, with one template and one target shape. Ultimatelly, we would like to do <em>statistics on shapes</em> (this is indeed Grenander’s motivation). Many people have worked in this area, but I didn’t review any of their work here; think of the post you’ve read as an entry-point to more advanced subjects within shape analysis. A cool example: Stefan Sommer constructs <a href="https://slides.com/stefansommer/geometry-stochastics-geometric-statistics">random walks in shape space</a>!</p>]]></content><author><name>Klas Modin</name></author><category term="undergraduate"/><category term="shape-analysis"/><summary type="html"><![CDATA[a brief introduction to the geometry of shape warping]]></summary></entry></feed>